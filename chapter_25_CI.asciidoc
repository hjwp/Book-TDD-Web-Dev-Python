[[chapter_25_CI]]
== CI: Continuous Integration

((("continuous integration (CI)", id="CI24")))
((("continuous integration (CI)", "benefits of")))
As our site grows, it takes longer and longer to run all of our functional tests.
If this continues, the danger is that we're going to stop bothering.((("CI", see="continuous integration")))

Rather than let that happen, we can automate the running of functional tests
by setting up "continuous integration", or CI.
That way, in day-to-day development,
we can just run the FT that we're working on at that time,
and rely on CI to run all the other tests automatically
and let us know if we've broken anything accidentally.


The unit tests should stay fast enough that we can keep running
the full suite locally, every few seconds.

NOTE: Continuous integration is another practice that was popularised by
    Kent Beck's
    https://martinfowler.com/bliki/ExtremeProgramming.html[extreme programming (XP)]
    movement in the 1990s.

As we'll see, one of the great frustrations of configuring CI
is that the feedback loop is so much slower than working locally.
As we go along, we'll look for ways to optimise for that where we can.

While debugging, we'll also touch on the theme of _reproducibility_. It's the idea that we want to be able to reproduce behaviours of our CI environment locally—in the same way that we try and make our production and dev environments as similar as possible.

[role="pagebreak-before less_space"]
=== CI in Modern Development Workflows

We use CI for a number of reasons:

* As mentioned, it can patiently run the full suite of tests,
  even if they've grown too large to run locally.

* It can act as a "gate" in your deployment/release process,
  to ensure that you never deploy code that isn't passing all the tests.

* In open source projects that use a "pull request" workflow,
  it's a way to ensure that any code submitted by potentially unknown
  contributors passes all your tests, before you consider merging it.

* It's (sadly) increasingly common in corporate environments
  to see this pull request process and its associated CI checks
  to be used as the default way for teams to merge all code changes.footnote:[
I say "sadly" because you _should_ be able to trust your colleagues,
not put them through a process designed for open source projects
to de-risk code contributions from random strangers on the internet.
Look up "trunk-based development"
if you want to see more old people shouting at clouds on this topic.]



=== Choosing a CI Service

((("continuous integration (CI)", "choosing a service")))
In the early days, CI would be implemented by configuring a server
(perhaps under a desk in the corner of the office)
with software on it that could pull down all the code from the main branch
at the end of each day, and scripts to compile all the code and run all the tests—a process that became known as a "build".
Then, each morning, developers would take a look at the results,
and deal with any broken builds.

As the practice spread, and feedback cycles grew faster,
CI software matured. CI has become a common cloud-based service,
designed to integrate with code hosting providers like GitHub—or even provided directly by the same providers.
GitHub has "GitHub Actions", and because it's like, _right there_,
it's probably the most popular choice for open source projects these days.
In a corporate environment, you might come across other solutions
like CircleCI, Travis CI, and GitLab.

It is still absolutely possible to download and self-host your own CI server;
in the first and second editions of this book,
I demonstrated the use of Jenkins, a popular tool at the time.
But the installation and subsequent admin/maintenance burden is not effort-free,
so for this edition I wanted to pick a service more like the kind of thing you're likely to encounter in your day job—while trying not to endorse the largest commercial provider.
There's nothing wrong with GitHub Actions!
It just doesn't need any _more_ help dominating the market.


So I've decided to use GitLab in this book.
It is absolutely a commercial service,
but it retains an open source version, and you can self-host it if you want to. The syntax (it's always YAML...) and core concepts are common across all providers,
so the things you learn here will be replicable in whichever service
you encounter in future.

Like most of the services out there, GitLab has a free tier,
which will work fine for our purposes.


=== Getting Our Code into GitLab

GitLab is primarily a code hosting service, like GitHub,
so the first thing to do is get our code up there.((("GitLab", "getting code into", id="ix_GitL")))


==== Signing Up

Head over to https://gitlab.com[GitLab.com], and sign up for a free account.

Then, head over to your profile page, and find the SSH Keys section,
and upload a copy of your public key.



==== Starting a Project

Then, use the New Project -> Create Blank Project option,
as in <<gitlab-new-blank-project>>. Feel free to name the project whatever you want;
you can see I've fancifully named mine with a "z".
I'm a free spirit, what can I say.

.Creating a new repo on GitLab
[[gitlab-new-blank-project]]
image::images/tdd3_2501.png["New Blank Project"]


==== Pushing Our Code Up Using Git Push

First, we set up GitLab as a "remote" for our project:

[role="skipme"]
[subs="specialcharacters,quotes"]
----
# substitute your username and project name as necessary
$ *git remote add gitlab git@gitlab.com:yourusername/superlists.git*
$ *git remote -v*
gitlab    git@gitlab.com:hjwp/superlistz.git (fetch)
gitlab    git@gitlab.com:hjwp/superlistz.git (push)
origin    git@github.com:hjwp/book-example.git (fetch)
origin    git@github.com:hjwp/book-example.git (push)
# (as you can see i already had a remote for github called 'origin')
----


Now we can push up our code with `git push`:

[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *git push gitlab*
Enumerating objects: 706, done.
Counting objects: 100% (706/706), done.
Delta compression using up to 11 threads
Compressing objects: 100% (279/279), done.
Writing objects: 100% (706/706), 918.72 KiB | 131.25 MiB/s, done.
Total 706 (delta 413), reused 682 (delta 408), pack-reused 0 (from 0)
remote: Resolving deltas: 100% (413/413), done.
To gitlab.com:hjwp/superlistz.git
 * [new branch]        main -> main
branch 'main' set up to track 'gitlab/main'.
----

If you refresh the GitLab UI, you should now see your code,
as in <<gitlab_files_ui>>.

.CI project files on GitLab
[[gitlab_files_ui]]
image::images/tdd3_2502.png["GitLab UI showing project files"]


=== Setting Up a First Cut of a CI Pipeline

The "pipeline" terminology was popularised by Dave Farley and Jez Humble
in their book _Continuous Delivery_ (Addison-Wesley).((("GitLab", "getting code into", startref="ix_GitL")))((("pipelines (CI)")))((("continuous integration (CI)", "setting up CI pipeline, first cut", id="ix_CIpipe1")))
The name alludes to the fact that a CI build typically has a series,
where the process flows from one to another.


Go to Build -> Pipelines, and you'll see a list of example templates.
When getting to know a new configuration language,
it's nice to be able to start with something that works,
rather than a blank slate. I chose the Python example template and made a few customisations,
but you could just as easily start from a blank slate and paste
what I have here (YAML, once again, folks!):




[role="sourcecode"]
..gitlab-ci.yml (ch25l001)
====
[source,yaml]
----
# Use the same image as our Dockerfile
image: python:slim

# These two settings let us cache pip-installed packages,
# it came from the default template
variables:
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
cache:
  paths:
    - .cache/pip

# "setUp" phase, before the main build
before_script:
  - python --version ; pip --version  # For debugging
  - pip install virtualenv
  - virtualenv .venv
  - source .venv/bin/activate

# This is the main build
test:
  script:
    - pip install -r requirements.txt  # <1>
    # unit tests
    - python src/manage.py test lists accounts  # <2>
    # (if those pass) all tests, incl. functional.
    - pip install selenium  # <3>
    - cd src && python manage.py test  # <4>
----
====

<1> We start by installing our core requirements.

<2> I've decided to run the unit tests first.
    This gives us an "early failure" if  there's any problem at this stage,
    and saves us from having to run—and more importantly, wait for—the FTs to run.

<3> Then we need Selenium for the functional tests.
    Again, I'm delaying this `pip install` until it's absolutely necessary,
    to get feedback as quickly as possible.

<4> And here is a full test run, including the functional tests.


TIP: It's a good idea in CI pipelines to try and run the quickest tests first,
    so that you can get feedback as quickly as possible.


You can use the GitLab web UI to edit your pipeline YAML,
and then when you save it, you can go check for results straight away.

But it is also just a file in your repo!
So as we go on through the chapter, you can also just edit it locally.
You'll need to commit it and then `git push` up to GitLab,
and then go check the Jobs section
in the Build UI to see the results((("continuous integration (CI)", "setting up CI pipeline", startref="ix_CIpipe1"))) of your changes:


[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *git push gitlab*
----


=== First Build!  (and First Failure)

// IDEA: consider deliberately forgetting to pip install selenium

Whichever way you click through the UI, you should be able to find your way
to see the output of the build job, as in <<gitlab_first_build>>.((("continuous integration (CI)", "building the pipeline", id="ix_CIbld")))((("GitLab", "building a CI pipeline in")))

.First build on GitLab
[[gitlab_first_build]]
image::images/tdd3_2503.png["GitLab UI showing the output of the first build"]


[role="pagebreak-before"]
Here's a selection of what I saw in the output console:


[role="skipme small-code"]
----
Running with gitlab-runner 17.7.0~pre.103.g896916a8 (896916a8)
  on green-1.saas-linux-small-amd64.runners-manager.gitlab.com/default
  JLgUopmM, system ID: s_deaa2ca09de7
Preparing the "docker+machine" executor 00:20
Using Docker executor with image python:latest ...
Pulling docker image python:latest ...
[...]
$ python src/manage.py test lists accounts
Creating test database for alias 'default'...
Found 55 test(s).
System check identified no issues (0 silenced).
................../builds/hjwp/book-example/.venv/lib/python3.13/site-packages/django/core
/handlers/base.py:61: UserWarning: No directory at: /builds/hjwp/book-example/src/static/
  mw_instance = middleware(adapted_handler)
.....................................
 ---------------------------------------------------------------------
Ran 53 tests in 0.129s
OK
Destroying test database for alias 'default'...
$ pip install selenium
Collecting selenium
  Using cached selenium-4.28.1-py3-none-any.whl.metadata (7.1 kB)
Collecting urllib3<3,>=1.26 (from urllib3[socks]<3,>=1.26->selenium)
[...]
Successfully installed attrs-25.1.0 certifi-2025.1.31 h11-0.14.0 idna-3.10 
outcome-1.3.0.post0 pysocks-1.7.1 selenium-4.28.1 sniffio-1.3.1 sortedcontainers-2.4.0 
trio-0.29.0 trio-websocket-0.12.1 typing_extensions-4.12.2 urllib3-2.3.0 
websocket-client-1.8.0 wsproto-1.2.0
$ cd src && python manage.py test
Creating test database for alias 'default'...
Found 63 test(s).
System check identified no issues (0 silenced).
......../builds/hjwp/book-example/.venv/lib/python3.13/site-packages/django/core/handlers
/base.py:61: UserWarning: No directory at: /builds/hjwp/book-example/src/static/
  mw_instance = middleware(adapted_handler)
...............................................EEEEEEEE
======================================================================
ERROR: test_layout_and_styling (functional_tests.test_layout_and_styling.
LayoutAndStylingTest.test_layout_and_styling)
 ---------------------------------------------------------------------
Traceback (most recent call last):
  File "/builds/hjwp/book-example/src/functional_tests/base.py", line 30, in setUp
    self.browser = webdriver.Firefox()
                   ~~~~~~~~~~~~~~~~~^^

[...]
selenium.common.exceptions.WebDriverException: Message: Process unexpectedly closed with 
status 255
 ---------------------------------------------------------------------
Ran 61 tests in 8.658s
FAILED (errors=8)

selenium.common.exceptions.WebDriverException: Message: Process unexpectedly closed with 
status 255
----

NOTE: If GitLab won't run your build at this point,
  you may need to go through some sort of identity-verification process.
  Check your profile page.
  
You can see we got through the unit tests,
and then in the full test run we have 8 errors out of 63 tests.
The FTs are all failing. I'm "lucky" because I've done this sort of thing many times before,
so I know what to expect:  it's failing because Firefox isn't installed
in the image we're using.((("Firefox", "installing in container image")))


Let's modify the script, and add an `apt install`.
Again we'll do it as late as possible:

[role="sourcecode"]
..gitlab-ci.yml (ch25l002)
====
[source,yaml]
----
# This is the main build
test:
  script:
    - pip install -r requirements.txt
    # unit tests
    - python src/manage.py test lists accounts
    # (if those pass) all tests, incl. functional.
    - apt update -y && apt install -y firefox-esr  # <1>
    - pip install selenium
    - cd src && python manage.py test
----
====

<1> We use the Debian Linux `apt` package manager to install Firefox.
    `firefox-esr` is the "extended support release",
    which is a more stable version of Firefox to test against.

[role="pagebreak-before"]
When you save that change (and commit and push if necessary),
the pipeline will run again.
If you wait a bit, you'll see we get a slightly different failure:


[role="skipme small-code"]
----
$ apt-get update -y && apt-get install -y firefox-esr
Get:1 http://deb.debian.org/debian bookworm InRelease [151 kB]
Get:2 http://deb.debian.org/debian bookworm-updates InRelease [55.4 kB]
Get:3 http://deb.debian.org/debian-security bookworm-security InRelease [48.0 kB]
[...]
The following NEW packages will be installed:
  adwaita-icon-theme alsa-topology-conf alsa-ucm-conf at-spi2-common
  at-spi2-core dbus dbus-bin dbus-daemon dbus-session-bus-common
  dbus-system-bus-common dbus-user-session dconf-gsettings-backend
  dconf-service dmsetup firefox-esr fontconfig fontconfig-config
[...]
Get:117 http://deb.debian.org/debian-security bookworm-security/main amd64
firefox-esr amd64 128.7.0esr-1~deb12u1 [69.8 MB]
[...]
Selecting previously unselected package firefox-esr.
Preparing to unpack .../105-firefox-esr_128.7.0esr-1~deb12u1_amd64.deb ...
Adding 'diversion of /usr/bin/firefox to /usr/bin/firefox.real by firefox-esr'
Unpacking firefox-esr (128.7.0esr-1~deb12u1) ...
[...]
Setting up firefox-esr (128.7.0esr-1~deb12u1) ...
update-alternatives: using /usr/bin/firefox-esr to provide
/usr/bin/x-www-browser (x-www-browser) in auto mode
[...]
======================================================================
ERROR: test_multiple_users_can_start_lists_at_different_urls
(functional_tests.test_simple_list_creation.NewVisitorTest.
test_multiple_users_can_start_lists_at_different_urls)
 ---------------------------------------------------------------------
Traceback (most recent call last):
  File "/builds/hjwp/book-example/src/functional_tests/base.py", line 30, in setUp
    self.browser = webdriver.Firefox()
                   ~~~~~~~~~~~~~~~~~^^
[...]
selenium.common.exceptions.WebDriverException: Message: Process unexpectedly
closed with status 1
 ---------------------------------------------------------------------
Ran 61 tests in 3.654s
FAILED (errors=8)
----

We can see Firefox installing OK, but we still get an error.
This time, it's exit code 1.

[role="pagebreak-before less_space"]
==== Trying to Reproduce a CI Error Locally

The cycle of "change _.gitlab-ci.yml_, push, wait for a build, check results"
is painfully slow. Let's see if we can reproduce this error locally.((("errors", "reproducing CI error locally")))

To reproduce the CI environment locally, I put together a quick Dockerfile,
by copy-pasting the steps in the `script` section and prefixing them with `RUN` commands:


[role="sourcecode"]
.infra/Dockerfile.ci (ch25l003)
====
[source,dockerfile]
----
FROM python:slim

RUN pip install virtualenv
RUN virtualenv .venv

# this won't work
# RUN source .venv/bin/activate
# use full path to venv instead.

COPY requirements.txt requirements.txt
RUN .venv/bin/pip install -r requirements.txt
RUN apt update -y && apt install -y firefox-esr
RUN .venv/bin/pip install selenium

COPY infra/debug-ci.py debug-ci.py
CMD .venv/bin/python debug-ci.py
----
====

And let's add a little debug script at _debug-ci.py_:


[role="sourcecode small-code"]
.infra/debug-ci.py (ch25l004)
====
[source,python]
----
from selenium import webdriver

# just try to open a selenium session
webdriver.Firefox().quit()
----
====

[role="pagebreak-before"]
We build and run it like this:

[role="skipme small-code"]
[subs="specialcharacters,macros"]
----
$ pass:specialcharacters,quotes[*docker build -f infra/Dockerfile.ci -t debug-ci . && \
  docker run -it debug-ci*]
[...]
 => [internal] load build definition from infra/Dockerfile.ci         0.0s
 => => transferring dockerfile: [...]
 => [internal] load metadata for docker.io/library/python:slim [...]
 => [1/8] FROM docker.io/library/python:slim@sha256:[...]
 => CACHED [2/8] RUN pip install virtualenv                           0.0s
 => CACHED [3/8] RUN virtualenv .venv                                 0.0s
 => CACHED [4/8] COPY requirements.txt requirements.txt               0.0s
 => CACHED [5/8] RUN .venv/bin/pip install -r requirements.txt        0.0s
 => CACHED [6/8] RUN apt update -y && apt install -y firefox-esr      0.0s
 => CACHED [7/8] RUN .venv/bin/pip install selenium                   0.0s
 => [8/8] COPY infra/debug-ci.py debug-ci.py                          0.0s
 => exporting to image                                                0.0s
 => => exporting layers                                               0.0s
 => => writing image sha256:[...]
 => => naming to docker.io/library/debug-ci                           0.0s
Traceback (most recent call last):
  File
  "//.venv/lib/python3.13/site-packages/selenium/webdriver/common/driver_finder.py",
  line 67, in _binary_paths
    output = SeleniumManager().binary_paths(self._to_args())
[...]
selenium.common.exceptions.WebDriverException: Message: Unsupported
platform/architecture combination: linux/aarch64

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "//debug-ci.py", line 4, in <module>
    webdriver.Firefox().quit()
    ~~~~~~~~~~~~~~~~~^^
[...]
selenium.common.exceptions.NoSuchDriverException: Message: Unable to obtain
driver for firefox; For documentation on this error, please visit:
https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location
----

You might not see this--that "Unsupported platform/architecture combination" error is spurious;
it's because I was on a Mac.  Let's try again with:

// SEBASTIAN: Might use extra sentence of explanation why being on Mac requires you to
// do a cross-build

[role="ignore-errors"]
[subs="specialcharacters,macros"]
----
$ pass:specialcharacters,quotes[*docker build -f infra/Dockerfile.ci -t debug-ci --platform=linux/amd64 . && \
  docker run --platform=linux/amd64 -it debug-ci*]
[...]
Traceback (most recent call last):
  File "//debug-ci.py", line 4, in <module>
    webdriver.Firefox().quit()
[...]
selenium.common.exceptions.WebDriverException: Message: Process unexpectedly
closed with status 1
----

OK, that's a reproduction of our issue.  But no further clues yet!


==== Enabling Debug Logs for Selenium/Firefox/Webdriver

Getting debug information out of Selenium can be a bit fiddly.((("logging", "enabling debug logs for Firefox/Selenium/Webdriver")))((("Webdriver", "enabling debug logs for")))((("Firefox", "enabling debug logs for")))((("Selenium", "enabling debug logs for")))
I tried two avenues: setting `options` and setting the `service`.
The former doesn't really work as far as I can tell,
but the latter does:

[role="sourcecode"]
.infra/debug-ci.py (ch25l005)
====
[source,python]
----
import subprocess

from selenium import webdriver

options = webdriver.FirefoxOptions()  # <1>
options.log.level = "trace"

service = webdriver.FirefoxService(  # <2>
    log_output=subprocess.STDOUT, service_args=["--log", "trace"]
)

# just try to open a selenium session
webdriver.Firefox(options=options, service=service).quit()
----
====

<1> This is how I attempted to increase the log level using `options`.
    I had to reverse-engineer it from the source code,
    and it doesn't seem to work anyway,
    but I thought I'd leave it here for future reference. There is some limited info in the
https://www.selenium.dev/documentation/webdriver/browsers/firefox/#log-output[Selenium docs].

<2> This is the `FirefoxService` config class,
    which _does_ seem to let you print some debug info.
    I'm configuring it to print to standard output.

Sure enough, we can see some output now!

[role="ignore-errors small-code"]
[subs="specialcharacters,macros"]
----
$ pass:specialcharacters,quotes[*docker build -f infra/Dockerfile.ci -t debug-ci --platform=linux/amd64 . && \
  docker run --platform=linux/amd64 -it debug-ci*]
[...]
1234567890111   geckodriver     INFO    Listening on 127.0.0.1:XXXX
1234567890112   webdriver::server       DEBUG   -> POST /session
{"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "firefox",
"acceptInsecureCerts": true, ... , "moz:firefoxOptions": {"binary":
"/usr/bin/firefox", "prefs": {"remote.active-protocols": 1}, "log": {"level":
"trace"}}}}}
1234567890111   geckodriver::capabilities       DEBUG   Trying to read firefox
version from ini files
1234567890111   geckodriver::capabilities       DEBUG   Trying to read firefox
version from binary
1234567890111   geckodriver::capabilities       DEBUG   Found version
128.10.1esr
1740029792102   mozrunner::runner       INFO    Running command:
MOZ_CRASHREPORTER="1" MOZ_CRASHREPORTER_NO_REPORT="1"
MOZ_CRASHREPORTER_SHUTDOWN="1" [...]
"--remote-debugging-port" [...]
"-no-remote" "-profile" "/tmp/rust_mozprofile[...]
1234567890111   geckodriver::marionette DEBUG   Waiting 60s to connect to
browser on 127.0.0.1
1234567890111   geckodriver::browser    TRACE   Failed to open
/tmp/rust_mozprofile[...]
1234567890111   geckodriver::marionette TRACE   Retrying in 100ms
Error: no DISPLAY environment variable specified
1234567890111   geckodriver::browser    DEBUG   Browser process stopped: exit
status: 1
1234567890112   webdriver::server       DEBUG   <- 500 Internal Server Error
{"value":{"error":"unknown error","message":"Process unexpectedly closed with
status 1","stacktrace":""}}
Traceback (most recent call last):
  File "//debug-ci.py", line 13, in <module>
    webdriver.Firefox(options=options, service=service).quit()
[...]
selenium.common.exceptions.WebDriverException: Message: Process unexpectedly
closed with status 1
----

// DAVID: Pasting this into an LLM gave some good suggestions.

Well, it wasn't immediately obvious what's going on there,
but I did eventually get a clue from the line that says `no DISPLAY environment variable specified`.

Out of curiosity, I thought I'd try running `firefox` directly:footnote:[
If you remember from <<chapter_09_docker>>, `docker run`
by default runs the command specified in `CMD`,
but you can override that by specifying a different command to run at the end of the parameter list.]


[role="ignore-errors"]
[subs="specialcharacters,quotes"]
----
$ *docker build -f infra/Dockerfile.ci -t debug-ci --platform=linux/amd64 . && \
  docker run --platform=linux/amd64 -it debug-ci firefox*
[...]
Error: no DISPLAY environment variable specified
----

Sure enough, the same error.


==== Enabling Headless Mode for Firefox

If you search around for this error,
you'll eventually find enough pointers to the answer:
Firefox is crashing because it can't find a display.((("headless mode")))((("Firefox", "enabling headless mode for")))
Servers are "headless", meaning they don't have a screen.
Thankfully, Firefox has a headless mode,
which we can enable by setting an environment variable,
`MOZ_HEADLESS`.

Let's confirm that locally. We'll use the `-e` flag for `docker run`:

[subs="specialcharacters,macros"]
----
$ pass:specialcharacters,quotes[*docker build -f infra/Dockerfile.ci -t debug-ci --platform=linux/amd64 . && \
  docker run -e MOZ_HEADLESS=1 --platform=linux/amd64 -it debug-ci*]
1234567890111   geckodriver     INFO    Listening on 127.0.0.1:43137
[...]
*** You are running in headless mode.
[...]
1234567890112   webdriver::server       DEBUG   Teardown [...]
1740030525996   Marionette      DEBUG   Closed connection 0
1234567890111   geckodriver::browser    DEBUG   Browser process stopped: exit
status: 0
1234567890112   webdriver::server       DEBUG   <- 200 OK [...]
----

It takes quite a long time to run,
and there's lots of debug out, but...it looks OK!
That's no longer an error.


Let's set that environment variable in our CI script:

[role="sourcecode"]
..gitlab-ci.yml (ch25l006)
====
[source,yaml]
----
variables:
  # Put pip-cache in home folder so we can use gitlab cache
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
  # Make Firefox run headless.
  MOZ_HEADLESS: "1"
----
====

TIP: Using a local Docker image to reproduce the CI environment
  is a hint that it might be worth investing time in running CI
  in a custom Docker image that you fully control;
  this is another way of improving _reproducibility_.
  We won't have time to go into detail in this book though.


And we'll see what happens when we do `git push gitlab` again.

[role="pagebreak-before"]
=== A Common Bugbear: Flaky Tests

Did it work for you?  For me, it _almost_ did.((("continuous integration (CI)", "building the pipeline", startref="ix_CIbld")))((("flaky tests")))
All but one of the FTs passed,
but I did see one unexpected error:


[role="skipme small-code"]
----
+ python manage.py test functional_tests
......F.
======================================================================
FAIL: test_can_start_a_todo_list
(functional_tests.test_simple_list_creation.NewVisitorTest)
 ---------------------------------------------------------------------
Traceback (most recent call last):
  File "...goat-book/functional_tests/test_simple_list_creation.py", line
38, in test_can_start_a_todo_list
    self.wait_for_row_in_list_table('2: Use peacock feathers to make a fly')
  File "...goat-book/functional_tests/base.py", line 51, in
wait_for_row_in_list_table
    raise e
  File "...goat-book/functional_tests/base.py", line 47, in
wait_for_row_in_list_table
    self.assertIn(row_text, [row.text for row in rows])
AssertionError: '2: Use peacock feathers to make a fly' not found in ['1: Buy
peacock feathers']
 ---------------------------------------------------------------------
----


Now, you might not see this error,
but it's common for the switch to CI to flush out some "flaky" tests—things that will fail intermittently.
In CI, a common cause is the "noisy neighbour" problem,
where the CI server might be much slower than your own machine,
thus flushing out some race conditions—or in this case,
just randomly hanging for a few seconds, taking us past the default timeout.


Let's give ourselves some tools to help debug though.


=== Taking Screenshots

((("continuous integration (CI)", "screenshots", id="CIscreen24")))
((("screenshots", id="screen24")))
((("debugging", "screenshots for", id="DBscreen24")))
To be able to debug unexpected failures that happen on a remote server,
it would be good to see a picture of the screen at the moment of the failure,
and maybe also a dump of the page's HTML.

We can do that using some custom logic in our FT class `tearDown`.
We'll need to do a bit of introspection of `unittest` internals
(a private attribute called `._outcome`)
but this will work:footnote:[...or at least until the next Python version.
Using private APIs is risky, but I couldn't find a better way.]


[role="sourcecode"]
.src/functional_tests/base.py (ch25l007)
====
[source,python]
----
import os
import time
from datetime import datetime
from pathlib import Path
[...]
MAX_WAIT = 5

SCREEN_DUMP_LOCATION = Path(__file__).absolute().parent / "screendumps"
[...]
class FunctionalTest(StaticLiveServerTestCase):
    def setUp(self):
        [...]

    def tearDown(self):
        if self._test_has_failed():
            if not SCREEN_DUMP_LOCATION.exists():
                SCREEN_DUMP_LOCATION.mkdir(parents=True)
            self.take_screenshot()
            self.dump_html()
        self.browser.quit()
        super().tearDown()

    def _test_has_failed(self):
        # slightly obscure but couldn't find a better way!
        return self._outcome.result.failures or self._outcome.result.errors
----
====

We first create a directory for our screenshots if necessary,
and then we take our screenshot and dump the HTML.
Let's see how those will work:

[role="sourcecode"]
.src/functional_tests/base.py (ch25l008)
====
[source,python]
----
    def take_screenshot(self):
        path = SCREEN_DUMP_LOCATION / self._get_filename("png")
        print("screenshotting to", path)
        self.browser.get_screenshot_as_file(str(path))

    def dump_html(self):
        path = SCREEN_DUMP_LOCATION / self._get_filename("html")
        print("dumping page HTML to", path)
        path.write_text(self.browser.page_source)
----
====

And finally, here's a way of generating a unique filename identifier,
which includes the name of the test and its class, as well as a timestamp:

[role="sourcecode small-code"]
.src/functional_tests/base.py (ch25l009)
====
[source,python]
----
    def _get_filename(self, extension):
        timestamp = datetime.now().isoformat().replace(":", ".")[:19]
        return (
            f"{self.__class__.__name__}.{self._testMethodName}-{timestamp}.{extension}"
        )
----
====

You can test this first locally by deliberately breaking one of the tests—with a `self.fail()` half-way through, for example—and you'll see something like this:


[role="dofirst-ch25l010"]
[subs="specialcharacters,quotes"]
----
$ *./src/manage.py test functional_tests.test_my_lists*
[...]
Fscreenshotting to ...goat-book/src/functional_tests/screendumps/MyListsTest.te
st_logged_in_users_lists_are_saved_as_my_lists-[...]
dumping page HTML to ...goat-book/src/functional_tests/screendumps/MyListsTest.
test_logged_in_users_lists_are_saved_as_my_lists-[...]
Fscreenshotting to ...goat-book/src/functional_tests/screendumps/MyListsTest.te
st_logged_in_users_lists_are_saved_as_my_lists-2025-02-18T11.29.00.png
dumping page HTML to ...goat-book/src/functional_tests/screendumps/MyListsTest.
test_logged_in_users_lists_are_saved_as_my_lists-2025-02-18T11.29.00.html
----

Why not try and open one of those files up?  It's kind of satisfying.


=== Saving Build Outputs (or Debug Files) as Artifacts

We also need to tell GitLab to "save" these files,
for us to be able to actually look at them.((("GitLab", "saving build outputs as artifacts")))((("artifacts")))
Those are called _artifacts_:

[role="sourcecode"]
..gitlab-ci.yml (ch25l012)
====
[source,yaml]
----
test:
  [...]

  script:
    [...]

  artifacts: # <1>
    when: always  # <2>
    paths: # <1>
      - src/functional_tests/screendumps/
----
====

<1> `artifacts` is the name of the key,
    and the `paths` argument is fairly self-explanatory.
    You can use wildcards here—more info in the https://docs.gitlab.com/ci/jobs/job_artifacts[GitLab docs].

<2> One thing the docs _didn't_ make obvious is that you need `when: always`,
    because otherwise it won't save artifacts for failed jobs.
    That was annoyingly hard to figure out!


In any case, that should work.
If you commit the code and then push it back to GitLab,
we should be able to see a new build job:

[role="dofirst-ch25l010-1"]
[subs="specialcharacters,quotes"]
----
$ *echo "src/functional_tests/screendumps" >> .gitignore*
$ *git commit -am "add screenshot on failure to FT runner"*
$ *git push*
----


In its output, we'll see the screenshots and HTML dumps being saved:


[role="skipme small-code"]
----
screendumps/LoginTest.test_can_get_email_link_to_log_in-window0-2014-01-22T17.45.12.html
Fscreenshotting to /builds/hjwp/book-example/src/functional_tests/screendumps/
NewVisitorTest.test_can_start_a_todo_list-2025-02-17T17.51.01.png
dumping page HTML to /builds/hjwp/book-example/src/functional_tests/screendumps/
NewVisitorTest.test_can_start_a_todo_list-2025-02-17T17.51.01.html
Not Found: /favicon.ico
.screenshotting to /builds/hjwp/book-example/src/functional_tests/screendumps/
NewVisitorTest.test_multiple_users_can_start_lists_at_different_urls-2025-02-17T17.
51.06.png
dumping page HTML to /builds/hjwp/book-example/src/functional_tests/screendumps/
NewVisitorTest.test_multiple_users_can_start_lists_at_different_urls-2025-02-17T17.51.
06.html
======================================================================
FAIL: test_can_start_a_todo_list (functional_tests.test_simple_list_creation.NewVisitorTest.
test_can_start_a_todo_list)
[...]
----


And to the right, some new UI options appear to Browse the artifacts,
as in <<gitlab_ui_for_browse_artifacts>>.

.Artifacts appear on the right of the build job
[[gitlab_ui_for_browse_artifacts]]
image::images/tdd3_2504.png["GitLab UI tab showing the option to browse artifacts"]


And if you navigate through, you'll see something like <<gitlab_ui_show_screenshot>>.

.Our screenshot in the GitLab UI, looking unremarkable
[[gitlab_ui_show_screenshot]]
image::images/tdd3_2505.png["GitLab UI showing a normal-looking screenshot of the site"]

// TODO: this errors if there are no screenshots.


=== If in Doubt, Try Bumping the Timeout!

((("", startref="CIscreen24")))
((("", startref="screen24")))
((("", startref="DBscreen24")))
((("continuous integration (CI)", "timeout bumping")))

Your build might be clear, but mine was still failing,
and those screenshots didn't offer any obvious clues.
Hmm. Well, when in doubt, bump the timeout—as the old adage goes:

[role="sourcecode skipme"]
.src/functional_tests/base.py
====
[source,python]
----
MAX_WAIT = 10
----
====

Then we can rerun the build by pushing, and confirm it now works.


=== A Successful Python Test Run

At this point, we should get a working pipeline (see <<gitlab_pipeline_success>>).

.A successful GitLab pipeline
[[gitlab_pipeline_success]]
image::images/tdd3_2506.png["GitLab UI showing a successful pipeline run"]



=== Running Our JavaScript Tests in CI

((("continuous integration (CI)", "setting up CI pipeline", startref="ix_CIpipe1")))((("continuous integration (CI)", "QUnit JavaScript tests", id="CIjs5")))
((("JavaScript testing", "in CI", secondary-sortas="CI", id="JSCI")))
There's a set of tests we almost forgot--the JavaScript tests.
Currently our "test runner" is an actual web browser.
To get them running in CI, we need a command-line test runner.

NOTE: Our JavaScript tests currently test the interaction
    between our code and the Bootstrap framework/CSS,
    so we still need a real browser to be able to make our
    visibility checks work.


Thankfully, the Jasmine docs point us straight towards the kind of tool we need:
https://github.com/jasmine/jasmine-browser-runner[Jasmine browser runner].


==== Installing Node.js

It's time to stop pretending we're not in the JavaScript game.
We're doing web development; that means we do JavaScript; that means we're going to end up with Node.js on our computers.((("Node.js", "installing")))
It's just the way it has to be.

Follow the instructions on the http://nodejs.org[Node.js home page].
It should guide you through installing the "node version manager" (nvm),
and then to getting the latest version of node:

[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *nvm install --lts*
Installing Node v22.17.0 (arm64)
[...]
$ *node -v*
v22.17.0
----


==== Installing and Configuring the Jasmine Browser Runner

The docs suggest we install it ((("Jasmine", "installing and configuring browser runner", id="ix_Jasbrwsrun")))((("browsers", "browser-based test runner (Jasmine)", id="ix_brwststrun")))like this,
and then run the `init` command to generate a default config file:

// IDEA: unskip. should be able to do some sort of rule=with-cd thingie
[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *cd src/lists/static*

$ *npm install --save-dev jasmine-browser-runner jasmine-core*
[...]
added 151 packages in 4s

$ *cat package.json*  # this is the equivalent of requirements.txt
{
  "devDependencies": {
    "jasmine-browser-runner": "^3.0.0",
    "jasmine-core": "^5.6.0"
  }
}

$ *ls node_modules/*
# will show several dozen directories

$ *npx jasmine-browser-runner init*
Wrote configuration to spec/support/jasmine-browser.mjs.
----

Well, we now have about a million files in _node_modules/_
(which is JavaScript's version of a virtualenv, essentially),
and we also have a new config file in _spec/support/jasmine-browser.mjs_. That's not the ideal place, because we've said our tests live in a folder called _tests_. So, let's move the config file in there:

[subs="specialcharacters,quotes"]
----
$ *mv spec/support/jasmine-browser.mjs tests/jasmine-browser-runner.config.mjs*
$ *rm -rf spec*
----

Then let's edit it slightly, to specify a few things correctly:

[role="sourcecode"]
.src/lists/static/tests/jasmine-browser-runner.config.mjs (ch25l013)
====
[source,js]
----
export default {
  srcDir: ".",  // <1>
  srcFiles: [
    "*.js"
  ],
  specDir: "tests",  // <2>
  specFiles: [
    "**/*[sS]pec.js"
  ],
  helpers: [
    "helpers/**/*.js"
  ],
  env: {
    stopSpecOnExpectationFailure: false,
    stopOnSpecFailure: false,
    random: true,
    forbidDuplicateNames: true
  },
  listenAddress: "localhost",
  hostname: "localhost",
  browser: {
    name: "headlessFirefox"  // <3>
  }
};
----
====
// DAVID: srcFiles was "**/*.js", should it be changed too?

<1> Our source files are in the current directory,
    _src/lists/static_—i.e., _lists.js_.

<2> Our spec files are in _tests/_.

<3> And here we say we want to use the headless
    version of Firefox.
    (We could have done this by setting `MOZ_HEADLESS`
    at the command line again, but this saves us from having to remember.)


Let's try running it now. We use the `--config` option to path it
the now non-standard path to the config file:

[role="skipme small-code"]
[subs="specialcharacters,quotes"]
----
$ *npx jasmine-browser-runner runSpecs \
  --config=tests/jasmine-browser-runner.config.mjs*
Jasmine server is running here: http://localhost:62811
Jasmine tests are here:         ...goat-book/src/lists/static/tests
Source files are here:          ...goat-book/src/lists/static
Running tests in the browser...
Randomized with seed 17843
Started
.F.

Failures:
1) Superlists tests error message should be hidden on input
  Message:
    Expected true to be false.
  Stack:
    <Jasmine>
    @http://localhost:62811/__spec__/Spec.js:46:40
    <Jasmine>

3 specs, 1 failure
Finished in 0.014 seconds
Randomized with seed 17843 (jasmine-browser-runner runSpecs --seed=17843)
----

Could be worse! One failure out of three specs. Unfortunately, it's the most important test:

[role="sourcecode currentcontents"]
.src/lists/static/tests/Spec.js
====
[source,python]
----
  it("should hide error message on input", () => {
    initialize(inputSelector);
    textInput.dispatchEvent(new InputEvent("input"));

    expect(errorMsg.checkVisibility()).toBe(false);
  });
----
====

Ah yes, if you remember, I said that the main reason we need to use a browser-based test runner
is because our visibility checks depend on the Bootstrap CSS framework.

In the HTML spec runner we've configured so far,
we load Bootstrap using a `<link>` tag:

[role="sourcecode currentcontents"]
.src/lists/static/tests/SpecRunner.html
====
[source,html]
----
  <!-- Bootstrap CSS -->
  <link href="../bootstrap/css/bootstrap.min.css" rel="stylesheet">
----
====

And here's how we load it for `jasmine-browser-runner`:

[role="sourcecode"]
.src/lists/static/tests/jasmine-browser-runner.config.mjs (ch25l014)
====
[source,js]
----
export default {
  srcDir: ".",
  srcFiles: [
    "*.js"
  ],
  specDir: "tests",
  specFiles: [
    "**/*[sS]pec.js"
  ],
  cssFiles: [  // <1>
    "bootstrap/css/bootstrap.min.css"  // <1>
  ],
  helpers: [
    "helpers/**/*.js"
  ],
----
====

<1> The `cssFiles` key is how you tell the runner to load, er, some CSS.
    I found that out in the https://jasmine.github.io/api/browser-runner/edge/Configuration.html[docs].


Let's give that a go...

[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *npx jasmine-browser-runner runSpecs --config=tests/jasmine-browser-runner.config.mjs*
Jasmine server is running here: http://localhost:62901
Jasmine tests are here:         .../goat-book/src/lists/static/tests
Source files are here:          .../goat-book/src/lists/static
Running tests in the browser...
Randomized with seed 06504
Started
...


3 specs, 0 failures
Finished in 0.016 seconds
Randomized with seed 06504 (jasmine-browser-runner runSpecs --seed=06504)
----

Hooray!  That works locally—let's get it into CI:


[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *cd -*  # go back to the project root
# add the package.json, which saves our node depenencies
$ *git add src/lists/static/package.json src/lists/static/package-lock.json*
# ignore the node_modules/ directory
$ *echo "node_modules/" >> .gitignore*
# and our config file
$ *git add src/lists/static/tests/jasmine-browser-runner.config.mjs*
$ *git add .gitignore*
$ *git commit -m "config for node + jasmine-browser-runner for JS tests"*
----
//015,016,017



==== Adding a Build Step for JavaScript

((("Jasmine", "installing and configuring browser runner", startref="ix_Jasbrwsrun")))((("browsers", "browser-based test runner (Jasmine)", startref="ix_brwststrun")))
We now want two different build steps,
so let's rename `test` to `test-python` and move all its
specific bits like `variables` and `before_script` inside it,
and then create a separate step called `test-js`,
with a similar structure:

[role="sourcecode"]
..gitlab-ci.yml (ch25l018)
====
[source,yaml]
----
test-python:
  # Use the same image as our Dockerfile
  image: python:slim  # <1>

  variables:  # <1>
    # Put pip-cache in home folder so we can use gitlab cache
    PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
    # Make Firefox run headless.
    MOZ_HEADLESS: "1"

  cache:  # <1>
    paths:
      - .cache/pip

  # "setUp" phase, before the main build
  before_script:  # <1>
    - python --version ; pip --version  # For debugging
    - pip install virtualenv
    - virtualenv .venv
    - source .venv/bin/activate

  script:
    - pip install -r requirements.txt
    # unit tests
    - python src/manage.py test lists accounts
    # (if those pass) all tests, incl. functional.
    - apt update -y && apt install -y firefox-esr
    - pip install selenium
    - cd src && python manage.py test

  artifacts:
    when: always
    paths:
      - src/functional_tests/screendumps/

test-js:  # <2>
  image: node:slim
  script:
    - apt update -y && apt install -y firefox-esr  # <3>
    - cd src/lists/static
    - npm install  # <4>
    - npx jasmine-browser-runner runSpecs
      --config=tests/jasmine-browser-runner.config.mjs  # <5>
----
====

<1> `image`, `variables`, `cache`, and `before_script` all move
    out of the top level and into the `test-python` step,
    as they're all specific to this step only now.

<2> Here's our new step, `test-js`.

<3> We install Firefox into the node image,
    just like we do for the Python one.

<4> We don't need to specify _what_ to `npm install`,
    because that's all in the _package-lock.json_ file.

<5> And here's our command to run the tests.


And slap me over the head with a wet fish if that doesn't pass on the first go!
See <<gitlab_pipeline_js_success>> for a successful pipeline run.


.Wow, there are those JavaScript tests, passing on the first attempt!
[[gitlab_pipeline_js_success]]
image::images/tdd3_2507.png["GitLab UI showing a successful pipeline run with JavaScript tests"]

((("", startref="CIjs5")))
((("", startref="JSCI")))



=== Tests Now Pass

And there we are!  A complete CI build featuring all of our tests! See <<gitlab_pipeline_overview_success.png>>.

.Here are both our jobs in all their green glory
[[gitlab_pipeline_overview_success.png]]
image::images/tdd3_2508.png["GitLab UI the pipeline overview, with both build jobs green"]


Nice to know that, no matter how lazy I get
about running the full test suite on my own machine, the CI server will catch me.
Another one of the Testing Goat's agents in cyberspace, watching over us...


.Alternatives: Woodpecker and Forgejo
*******************************************************************************

I want to give a shout out to https://woodpecker-ci.org[Woodpecker CI]
and https://forgejo.org[Forgejo], two of the newer self-hosted CI options.
And while I'm at it, to https://www.jenkins.io[Jenkins],
which did a great job for the first and second editions,
and still does for many people.((("continuous integration (CI)", "self-hosted CI options")))

// CSANAD: I just found framagit.org by Framasoft. Maybe we could mention them? Although
// it might be important to ask them first, in case they need to handle the
// expected additional traffic.

If you want true independence from overly commercial interests,
then self-hosted is the way to go.
You'll need your own server for both of these.

I tried both, and managed to get them working within an hour or two.
Their documentation is good.

If you do decide to give them a go, I'd say,
be a bit cautious about security options. For example, you might decide you don't want any old person from the internet
to be able to sign up for an account on your server:


[role="skipme"]
----
DISABLE_REGISTRATION: true
----

But more power to you for giving it a go!

*******************************************************************************


=== Some Things We Didn't Cover

CI is a big topic and, inevitably, I couldn't cover everything.
Here's a few pointers to things you might want to learn about.

==== Defining a Docker Image for CI

We spent quite a bit of time debugging—for example, the unhelpful messages
when Firefox wasn't installed.((("continuous integration (CI)", "defining Docker image for")))((("Docker", "defining container image for CI")))
Just as we did when preparing our deployment, it's a big help having an environment that you can run on your local machine
that's as close as possible to what you have remotely; that's why we chose to use Docker image.

In CI, our tests also run a Docker image (`python:slim` and `node:slim`),
so one common pattern is to define a Docker image within your repo that you'll use for CI.
Ideally, it should also be as similar as possible to the one you use in production!
A typical solution here is to use multistage Docker builds—with a base stage, a prod stage, and a dev/CI stage.
In our case, the last stage would have Firefox, Selenium,
and other test-only dependencies in it, which we don't need for prod.

You can then run your tests locally inside the same Docker image that's used in CI.((("reproducibility")))


TIP: _Reproducibility_ is one of the key attributes we're aiming for.
    The more your project grows in complexity,
    the more it's worth investing in minimising the differences
    between local dev, CI, and prod.


==== Caching

We touched on the use of caches in CI for the `pip` download cache,
but as CI pipelines grow in maturity,
you'll find you can make more and more use of caching. ((("caching", "caches in CI")))For example, it might be a good idea to cache your _node_modules/_
directory.

It's a topic for another time, but this is yet another way
of trying to speed up the feedback cycle.


==== Automated Deployment, aka Continuous Delivery (CD)

The natural next step is to finish our journey into automation,
and set up a pipeline that will deploy our code all the way to production,
each time we push code...as long as the tests pass!((("continuous delivery (CD)")))((("automated deployment", seealso="continuous deploymect")))((("deployment", "continuous delivery")))

I work through an example of how to do that in the https://www.obeythetestinggoat.com/book/appendix_CD.html[Online Appendix: Continuous Deployment (CD)]. If you're feeling inspired, I'd encourage you to take a look.

Now, onto our last chapter of coding, everyone!


.Best Practices for CI (Including Selenium Tips)
*******************************************************************************

Set up CI as soon as possible for your project.::
    As soon as your functional tests take more than a few seconds to run,
    you'll find yourself avoiding running them.
    Give this job to a CI server,
    to make sure that all your tests are being run somewhere.
    ((("Selenium", "best CI practices")))
    ((("continuous integration (CI)", "tips")))

Optimise for fast feedback.::
    CI feedback loops can be frustratingly slow.
    Optimising things to get results quicker is worth the effort.
    Run your fastest tests first,
    and use caches to try to minimise time spent on, for example, dependency installation.

Set up screenshots and HTML dumps for failures.::
    Debugging test failures is easier if you can see what the page looked
    like when the failure occurred.  This is particularly useful for debugging
    CI failures, but it's also very useful for tests that you run locally.
    ((("screenshots")))
    ((("debugging", "screenshots for")))
    ((("HTML", "screenshot dumps")))

Be prepared to bump your timeouts.::
    A CI server may not be as speedy as your laptop—especially if it's under load, running multiple tests at the same time.
    Be prepared to be even more generous with your timeouts,
    in order to minimise the chance of random failures.
    ((("flaky tests")))

Take the next step, CD (continuous deployment).::
    Once we're running tests automatically,
    we can take the next step, which is to automate our deployments
    (when the tests pass). See the https://www.obeythetestinggoat.com/book/appendix_CD.html[Online Appendix: Continuous Deployment (CD)].
    ((("continuous deployment (CD)")))

*******************************************************************************

