[[chapter_13_organising_test_files]]
== Splitting Our Tests into Multiple Files, and a Generic Wait Helper

.A Note for Early Release Readers
****
With Early Release ebooks, you get books in their earliest form—the author's
raw and unedited content as they write—so you can take advantage of these
technologies long before the official release of these titles.

This will be the 13th chapter of the final book.
The GitHub repo is available at
https://github.com/hjwp/book-example/tree/chapter_13_organising_test_files

If you have comments about how we might improve the content and/or examples in this book,
or if you notice missing material within this chapter,
please reach out to the author at obeythetestinggoat@gmail.com.
****

Back to local development!
The next feature we might like to implement is a little input validation.
But as we start writing new tests, we'll notice that
it's getting hard to find our way around a single _functional_tests.py_, and _tests.py_,
so we'll reorganise them into multiple files--a little refactor of our tests, if you will.

We'll also build a generic explicit wait helper.



=== Start on a Validation FT: Preventing Blank Items

((("list items", id="list12")))
((("user interactions", "preventing blank items", id="UIblank12")))
((("blank items, preventing", id="blank12")))
((("form data validation", "preventing blank items", id="FDVblank12")))
((("validation", see="form data validation; model-level validation")))
((("functional tests (FTs)", "for validation", secondary-sortas="validation", id="FTvalidat12")))
As our first few users start using the site,
we've noticed they sometimes make mistakes that mess up their lists,
like accidentally submitting blank list items,
or inputting two identical items to a list.
Computers are meant to help stop us from making silly mistakes,
so let's see if we can get our site to help.

Here's the outline of the new FT method which we will add to
`NewVisitorTestCase`:


[role="sourcecode"]
.src/functional_tests/tests.py (ch13l001)
====
[source,python]
----
def test_cannot_add_empty_list_items(self):
    # Edith goes to the home page and accidentally tries to submit
    # an empty list item. She hits Enter on the empty input box

    # The home page refreshes, and there is an error message saying
    # that list items cannot be blank

    # She tries again with some text for the item, which now works

    # Perversely, she now decides to submit a second blank list item

    # She receives a similar warning on the list page

    # And she can correct it by filling some text in
    self.fail("write me!")
----
====


That's all very well, but before we go any further--our
functional tests file is beginning to get a little crowded.
Let's split it out into several files, in which each has a single test method.


Remember that functional tests are closely linked to "user stories" and features.
One way of organising your FTs might be to have one per high-level feature.

We'll also have one base test class which they can all inherit from.  Here's
how to get there step by step.


==== Skipping a Test

NOTE: We're back to local development now.
    Make sure that the `TEST_SERVER` environment variable is unset by executing
    the command `unset TEST_SERVER` from the terminal.

((("unittest module", "skip test decorator")))
((("refactoring")))
((("decorators", "skip test decorator")))
It's always nice, when doing refactoring, to have a fully passing test suite.
We've just written a test with a deliberate failure.
Let's temporarily switch it off, using a decorator called "skip" from `unittest`:

[role="sourcecode"]
.src/functional_tests/tests.py (ch13l001-1)
====
[source,python]
----
from unittest import skip
[...]

    @skip
    def test_cannot_add_empty_list_items(self):
----
====

This tells the test runner to ignore this test.
You can see it works--if we rerun the tests,
you'll see it's a pass, but it explicitly mentions the skipped test:

[subs="specialcharacters,quotes"]
----
$ *python src/manage.py test functional_tests*
[...]
Ran 4 tests in 11.577s
OK (skipped=1)
----

WARNING: Skips are dangerous--you need to remember
    to remove them before you commit your changes back to the repo.
    This is why line-by-line reviews of each of your diffs are a good idea!


.Don't Forget the "Refactor" in "Red, Green, Refactor"
**********************************************************************

((("Test-Driven Development (TDD)", "concepts", "Red/Green/Refactor")))
((("Red/Green/Refactor")))
A criticism that's sometimes levelled at TDD is that
it leads to badly architected code,
as the developer just focuses on getting tests to pass
rather than stopping to think about how the whole system should be designed.
I think it's slightly unfair.

_TDD is no silver bullet_.
You still have to spend time thinking about good design.
But what often happens is that people forget the "Refactor" in "Red, Green, Refactor".
The methodology allows you to throw together any old code to get your tests to pass,
but it _also_ asks you to then spend some time refactoring it to improve its design.
Otherwise, it's too easy to allow
https://martinfowler.com/bliki/TechnicalDebtQuadrant.html["technical debt"]
to build up.

Often, however, the best ideas for how to refactor code don't occur to you straight away.
They may occur to you days, weeks, even months after you wrote a piece of code,
when you're working on something totally unrelated
and you happen to see some old code again with fresh eyes.
But if you're halfway through something else,
should you stop to refactor the old code?

The answer is that it depends.
In the case at the beginning of the chapter,
we haven't even started writing our new code.
We know we are in a working state,
so we can justify putting a skip on our new FT
(to get back to fully passing tests)
and do a bit of refactoring straight away.

Later in the chapter we'll spot other bits of code we want to alter.
In those cases, rather than taking the risk
of refactoring an application that's not in a working state,
we'll make a note of the thing we want to change on our scratchpad
and wait until we're back to a fully passing test suite before refactoring.

Kent Beck has a book-length exploration of the tradeoffs
of refactor-now vs refactor-later, called
https://www.oreilly.com/library/view/tidy-first/9781098151232/[Tidy First?]
**********************************************************************



=== Splitting Functional Tests Out into Many Files


((("functional tests (FTs)", "splitting into many files", id="FTsplit12")))
((("test files", "splitting FTs into many")))
We start putting each test into its own class, still in the same file:


[role="sourcecode"]
.src/functional_tests/tests.py (ch13l002)
====
[source,python]
----
class FunctionalTest(StaticLiveServerTestCase):
    def setUp(self):
        [...]
    def tearDown(self):
        [...]
    def wait_for_row_in_list_table(self, row_text):
        [...]


class NewVisitorTest(FunctionalTest):
    def test_can_start_a_todo_list(self):
        [...]
    def test_multiple_users_can_start_lists_at_different_urls(self):
        [...]


class LayoutAndStylingTest(FunctionalTest):
    def test_layout_and_styling(self):
        [...]


class ItemValidationTest(FunctionalTest):
    @skip
    def test_cannot_add_empty_list_items(self):
        [...]
----
====


At this point we can rerun the FTs and see they all still work:

----
Ran 4 tests in 11.577s

OK (skipped=1)
----

That's labouring it a little bit,
and we could probably get away with doing this stuff in fewer steps,
but, as I keep saying, practising the step-by-step method on the easy cases
makes it that much easier when we have a complex case.

Now we switch from a single tests file to using one for each class, and one
"base" file to contain the base class all the tests will inherit from.  We'll
make four copies of 'tests.py', naming them appropriately, and then delete the
parts we don't need from each:

[subs="specialcharacters,quotes"]
----
$ *git mv src/functional_tests/tests.py src/functional_tests/base.py*
$ *cp src/functional_tests/base.py src/functional_tests/test_simple_list_creation.py*
$ *cp src/functional_tests/base.py src/functional_tests/test_layout_and_styling.py*
$ *cp src/functional_tests/base.py src/functional_tests/test_list_item_validation.py*
----

_base.py_ can be cut down to just the `FunctionalTest` class.
We leave the helper method on the base class,
because we suspect we're about to reuse it in our new FT:

[role="sourcecode"]
.src/functional_tests/base.py (ch13l003)
====
[source,python]
----
import os
import time

from django.contrib.staticfiles.testing import StaticLiveServerTestCase
from selenium import webdriver
from selenium.common.exceptions import WebDriverException
from selenium.webdriver.common.by import By

MAX_WAIT = 5


class FunctionalTest(StaticLiveServerTestCase):
    def setUp(self):
        [...]
    def tearDown(self):
        [...]
    def wait_for_row_in_list_table(self, row_text):
        [...]
----
====

NOTE: Keeping helper methods in a base `FunctionalTest` class
    is one useful way of preventing duplication in FTs.
    Later in the book (in <<chapter_26_page_pattern>>) we'll use the "Page pattern",
    which is related, but prefers composition over inheritance--always a good thing.

Our first FT is now in its own file,
and should be just one class and one test method:

[role="sourcecode"]
.src/functional_tests/test_simple_list_creation.py (ch13l004)
====
[source,python]
----
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

from .base import FunctionalTest


class NewVisitorTest(FunctionalTest):
    def test_can_start_a_todo_list(self):
        [...]
    def test_multiple_users_can_start_lists_at_different_urls(self):
        [...]
----
====


I used a relative import (`from .base`).
Some people like to use them a lot in Django code
(e.g., your views might import models using `from .models import List`,
instead of `from list.models`).
Ultimately this is a matter of personal preference.
I prefer to use relative imports only when I'm super-super confident
that the relative position of the thing I'm importing won't change.
That applies in this case because I know for sure
all the tests will sit next to _base.py_, which they inherit from.



The layout and styling FT should now be one file and one class:

[role="sourcecode"]
.src/functional_tests/test_layout_and_styling.py (ch13l005)
====
[source,python]
----
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

from .base import FunctionalTest


class LayoutAndStylingTest(FunctionalTest):
        [...]
----
====


Lastly our new validation test is in a file of its own too:


[role="sourcecode"]
.src/functional_tests/test_list_item_validation.py (ch13l006)
====
[source,python]
----
from unittest import skip

from selenium.webdriver.common.by import By  # <1>
from selenium.webdriver.common.keys import Keys  # <1>

from .base import FunctionalTest


class ItemValidationTest(FunctionalTest):
    @skip
    def test_cannot_add_empty_list_items(self):
        [...]
----
====

<1> These two will be marked as "unused imports" for now but
    that's ok, we'll use them shortly

And we can test that everything worked
by rerunning `manage.py test functional_tests`,
and checking once again that all four tests are run:

----
Ran 4 tests in 11.577s

OK (skipped=1)
----

((("", startref="FTsplit12")))Now
we can remove our skip:

[role="sourcecode"]
.src/functional_tests/test_list_item_validation.py (ch13l007)
====
[source,python]
----
class ItemValidationTest(FunctionalTest):
    def test_cannot_add_empty_list_items(self):
        [...]
----
====


=== Running a Single Test File

((("functional tests (FTs)", "running single test files")))
((("test files", "running single")))
As a side bonus, we're now able to run an individual test file, like this:

[subs="specialcharacters,quotes"]
----
$ *python src/manage.py test functional_tests.test_list_item_validation*
[...]
AssertionError: write me!
----

Brilliant--no need to sit around waiting for all the FTs
when we're only interested in a single one.
Although we need to remember to run all of them now and again, to check for regressions.
Later in the book we'll set up a Continuous Integration (CI) server to run all the tests automatically,
for example every time we push to the main branch.
For now, a good prompt for running all the tests is "just before you do a commit",
so let's get into that habit now:

[subs="specialcharacters,quotes"]
----
$ *git status*
$ *git add src/functional_tests*
$ *git commit -m "Moved FTs into their own individual files"*
----

Great.  We've split our functional tests nicely out into different files.
Next we'll start writing our FT, but before long, as you may be guessing,
we'll do something similar to our unit test files.
((("", startref="list12")))
((("", startref="blank12")))
((("", startref="UIblank12")))
((("", startref="FDVblank12")))
((("", startref="FTvalidat12")))



=== A New Functional Test Tool: A Generic Explicit Wait Helper

((("implicit and explicit waits")))
((("explicit and implicit waits")))
((("functional tests (FTs)", "implicit/explicit waits and time.sleeps")))
((("generic explicit wait helper", id="gewhelper12")))
First let's start implementing the test, or at least the beginning of it:


[role="sourcecode"]
.src/functional_tests/test_list_item_validation.py (ch13l008)
====
[source,python]
----
def test_cannot_add_empty_list_items(self):
    # Edith goes to the home page and accidentally tries to submit
    # an empty list item. She hits Enter on the empty input box
    self.browser.get(self.live_server_url)
    self.browser.find_element(By.ID, "id_new_item").send_keys(Keys.ENTER)

    # The home page refreshes, and there is an error message saying
    # that list items cannot be blank
    self.assertEqual(
        self.browser.find_element(By.CSS_SELECTOR, ".invalid-feedback").text,  #<1>
        "You can't have an empty list item",  #<2>
    )

    # She tries again with some text for the item, which now works
    self.fail("finish this test!")
    [...]
----
====

This is how we might write the test naively:

<1> We specify we're going to use a CSS class called `.invalid-feedback` to mark our
    error text.  We'll see that Bootstrap has some useful styling for those.

<2> And we can check that our error displays the message we want.

But can you guess what the potential problem is with the test as it's written
now?

OK, I gave it away in the section header, but whenever we do something
that causes a page refresh, we need an explicit wait; otherwise, Selenium
might go looking for the `.invalid-feedback` element before the page has had a
chance to load.

TIP: Whenever you submit a form with `Keys.ENTER`
    or click something that is going to cause a page to load,
    you probably want an explicit wait for your next assertion.


Our first explicit wait was built into a helper method.  For this one, we
might decide that building a specific helper method is overkill at this stage,
but it might be nice to have some generic way of saying, in our tests, "wait
until this assertion passes".  Something like this:


[role="sourcecode"]
.src/functional_tests/test_list_item_validation.py (ch13l009)
====
[source,python]
----
[...]
    # The home page refreshes, and there is an error message saying
    # that list items cannot be blank
    self.wait_for(
        lambda: self.assertEqual(  #<1>
            self.browser.find_element(By.CSS_SELECTOR, ".invalid-feedback").text,
            "You can't have an empty list item",
        )
    )
----
====

<1> Rather than calling the assertion directly,
    we wrap it in a lambda function,
    and we pass it to a new helper method we imagine called `wait_for`.

NOTE: If you've never seen lambda functions in Python before,
    see <<lamdbafunct>>.

So how would this magical `wait_for` method work?
Let's head over to _base.py_, make a copy of our existing `wait_for_row_in_list_table` method,
and we'll adapt it slightly:


[role="sourcecode"]
.src/functional_tests/base.py (ch13l010)
====
[source,python]
----
    def wait_for(self, fn):  #<1>
        start_time = time.time()
        while True:
            try:
                table = self.browser.find_element(By.ID, "id_list_table")  #<2>
                rows = table.find_element(By.TAG_NAME, "tr")
                self.assertIn(row_text, [row.text for row in rows])
                return
            except (AssertionError, WebDriverException):
                if time.time() - start_time > MAX_WAIT:
                    raise
                time.sleep(0.5)
----
====

<1> We make a copy of the method, but we name it `wait_for`,
    and we change its argument.  It is expecting to be passed a function.

<2> For now we've still got the old code that's checking table rows.
    How to transform this into something that works
    for any generic `fn` that's been passed in?

Like this:

[[self.wait-for]]
[role="sourcecode"]
.src/functional_tests/base.py (ch13l011)
====
[source,python]
----
    def wait_for(self, fn):
        start_time = time.time()
        while True:
            try:
                return fn()  #<1>
            except (AssertionError, WebDriverException):
                if time.time() - start_time > MAX_WAIT:
                    raise
                time.sleep(0.5)
----
====

<1> The body of our try/except,
    instead of being the specific code for examining table rows,
    just becomes a call to the function we passed in.
    We also `return` its result,
    to be able to exit the loop immediately if no exception is raised.

[[lamdbafunct]]
.Lambda Functions
*******************************************************************************

((("lambda functions")))
((("Python 3", "lambda functions")))
`lambda` in Python is the syntax for making a one-line, throwaway function--it
saves you from having to use `def...():` and an indented block:

[role="skipme"]
[source,python]
----
>>> myfn = lambda x: x+1
>>> myfn(2)
3
>>> myfn(5)
6
>>> adder = lambda x, y: x + y
>>> adder(3, 2)
5
----

In our case, we're using it to transform a bit of code,
that would otherwise be executed immediately,
into a function that we can pass as an argument, and that can be executed later,
and multiple times:

[role="skipme"]
[source,python]
----
>>> def addthree(x):
...     return x + 3
...
>>> addthree(2)
5
>>> myfn = lambda: addthree(2)  # note addthree is not called immediately here
>>> myfn
<function <lambda> at 0x7f3b140339d8>
>>> myfn()
5
>>> myfn()
5
----

*******************************************************************************

Let's see our funky `wait_for` helper in action:


[subs="macros,verbatim"]
----
$ pass:quotes[*python src/manage.py test functional_tests.test_list_item_validation*]
[...]

======================================================================
ERROR: test_cannot_add_empty_list_items (functional_tests.test_list_item_valida
tion.ItemValidationTest.test_cannot_add_empty_list_items)
 ---------------------------------------------------------------------
[...]
Traceback (most recent call last):
  File "...goat-book/src/functional_tests/test_list_item_validation.py", line
16, in test_cannot_add_empty_list_items
    self.wait_for(<1>
  File "...goat-book/src/functional_tests/base.py", line 25, in wait_for
    return fn()<2>
           ^^^^
  File "...goat-book/src/functional_tests/test_list_item_validation.py", line
18, in <lambda><3>
    self.browser.find_element(By.CSS_SELECTOR, ".invalid-feedback").text,<3>
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[...]
selenium.common.exceptions.NoSuchElementException: Message: Unable to locate
element: .invalid-feedback; [...]

 ---------------------------------------------------------------------
Ran 1 test in 10.575s

FAILED (errors=1)
----

The order of the traceback is a little confusing, but we can more or less follow
through what happened:

<1> In our FT, we call our `self.wait_for` helper, where we pass
    the `lambda`-ified version of `assertEqual`.

<2> We go into `self.wait_for` in _base.py_,
    where we're calling (and returning) `fn()`, which refers to the passed
    lambda function encapsulating our test assertion.

<3> To explain where the exception has actually come from,
    the traceback takes us back into _test_list_item_validation.py_
    and inside the body of the `lambda` function,
    and tells us that it was attempt to find the `.invalid-feedback` element
    that failed.


((("functional programming")))
We're into the realm of functional programming now,
passing functions as arguments to other functions,
and it can be a little mind-bending.
I know it took me a little while to get used to!
Have a couple of read-throughs of this code,
and the code back in the FT, to let it sink in;
and if you're still confused, don't worry about it too much,
and let your confidence grow from working with it.
We'll use it a few more times in this book
and make it even more functionally fun, you'll see.
((("", startref="gewhelper12")))



=== Finishing Off the FT

We'll finish off the FT like this:

[role="sourcecode"]
.src/functional_tests/test_list_item_validation.py (ch13l012)
====
[source,python]
----
    # The home page refreshes, and there is an error message saying
    # that list items cannot be blank
    self.wait_for(
        lambda: self.assertEqual(
            self.browser.find_element(By.CSS_SELECTOR, ".invalid-feedback").text,
            "You can't have an empty list item",
        )
    )

    # She tries again with some text for the item, which now works
    self.browser.find_element(By.ID, "id_new_item").send_keys("Purchase milk")
    self.browser.find_element(By.ID, "id_new_item").send_keys(Keys.ENTER)
    self.wait_for_row_in_list_table("1: Purchase milk")

    # Perversely, she now decides to submit a second blank list item
    self.browser.find_element(By.ID, "id_new_item").send_keys(Keys.ENTER)

    # She receives a similar warning on the list page
    self.wait_for(
        lambda: self.assertEqual(
            self.browser.find_element(By.CSS_SELECTOR, ".invalid-feedback").text,
            "You can't have an empty list item",
        )
    )

    # And she can correct it by filling some text in
    self.browser.find_element(By.ID, "id_new_item").send_keys("Make tea")
    self.browser.find_element(By.ID, "id_new_item").send_keys(Keys.ENTER)
    self.wait_for_row_in_list_table("2: Make tea")
----
====



.Helper Methods in FTs
*******************************************************************************

((("functional tests (FTs)", "helper methods in")))
((("helper methods")))
((("self.wait_for helper method")))
((("wait_for_row_in_list_table helper method")))
We've got two helper methods now,
our generic `self.wait_for` helper, and `wait_for_row_in_list_table`.
The former is a general utility--any of our FTs might need to do a wait.

The second also helps prevent duplication across your functional test code.
The day we decide to change the implementation of how our list table works,
we want to make sure we only have to change our FT code in one place,
not in dozens of places across loads of FTs...

See also <<chapter_26_page_pattern>> and <<appendix_bdd>> for more on structuring
your FT code.
*******************************************************************************


I'll let you do your own "first-cut FT" commit.


=== Refactoring Unit Tests into Several Files


((("unit tests", "refactoring into several files")))
((("refactoring")))
((("test files", "splitting unit tests into several")))
When we (finally!) start coding our solution,
we're going to want to add another test for our _models.py_.
Before we do so, it's time to tidy up our unit tests
in a similar way to the functional tests.

A difference will be that, because the `lists` app contains real application code
as well as tests, we'll separate out the tests into their own folder:

[subs=""]
----
$ <strong>mkdir src/lists/tests</strong>
$ <strong>touch src/lists/tests/__init__.py</strong>
$ <strong>git mv src/lists/tests.py src/lists/tests/test_all.py</strong>
$ <strong>git status</strong>
$ <strong>git add src/lists/tests</strong>
$ <strong>python src/manage.py test lists</strong>
[...]
Ran 10 tests in 0.034s

OK
$ <strong>git commit -m "Move unit tests into a folder with single file"</strong>
----

If you get a message saying "Ran 0 tests",
you probably forgot to add the dunderinit.footnote:[
"Dunder" is shorthand for double-underscore,
so "dunderinit" means +++<i>__init__.py</i>+++.]
It needs to be there for the tests folder to be recognised as a regular python package,footnote:[
Without the dunderinit, a folder with python files in is called a
https://realpython.com/python-namespace-package/[namespace package].
Usually they are exactly the same as regular packages (which _do_ have a +++<i>__init__.py</i>+++),
but the Django test runner does not recognise them.]
and thus discovered by the test runner.

Now we turn _test_all.py_ into two files,
one called _test_views.py_, which will only contains view tests,
and one called _test_models.py_.
I'll start by making two copies:


[subs="specialcharacters,quotes"]
----
$ *git mv src/lists/tests/test_all.py src/lists/tests/test_views.py*
$ *cp src/lists/tests/test_views.py src/lists/tests/test_models.py*
----

And strip _test_models.py_ down
to being just the one test:

[role="sourcecode"]
.src/lists/tests/test_models.py (ch13l016)
====
[source,python]
----
from django.test import TestCase

from lists.models import Item, List


class ListAndItemModelsTest(TestCase):
        [...]
----
====

Whereas _test_views.py_ just loses one class:

[role="sourcecode"]
.src/lists/tests/test_views.py (ch13l017)
====
[source,diff]
----
--- a/src/lists/tests/test_views.py
+++ b/src/lists/tests/test_views.py
33 +74,3 @@ class NewItemTest(TestCase):
         )

         self.assertRedirects(response, f"/lists/{correct_list.id}/")
-
-
-class ListAndItemModelsTest(TestCase):
-    def test_saving_and_retrieving_items(self):
[...]
----
====

We rerun the tests to check that everything is still there:

[subs="specialcharacters,quotes"]
----
$ *python src/manage.py test lists*
[...]
Ran 10 tests in 0.040s

OK
----

Great!   That's another small, working step:

[subs="specialcharacters,quotes"]
----
$ *git add src/lists/tests*
$ *git commit -m "Split out unit tests into two files"*
----


NOTE: Some people like to make their unit tests into a tests folder
    straight away, as soon as they start a project. That's a perfectly good idea;
    I just thought I'd wait until it became necessary,
    to avoid doing too much housekeeping all in the first chapter!


Well, that's our FTs and unit test nicely reorganised.  In the next chapter
we'll get down to some validation proper.

[role="pagebreak-before less_space"]
.Tips on Organising Tests and Refactoring
*******************************************************************************

Use a tests folder::
    Just as you use multiple files to hold your application code, you should
    split your tests out into multiple files.
    * For functional tests, group them into tests for a particular feature or
      user story.
    * For unit tests, use a folder called 'tests', with a +++<i>__init__.py</i>+++.
    * You probably want a separate test file for each tested source code
      file. For Django, that's typically 'test_models.py', 'test_views.py', and
      'test_forms.py'.
    * Have at least a placeholder test for 'every' function and class.
    ((("test files", "organizing and refactoring")))

Don't forget the "Refactor" in "Red, Green, Refactor"::
    The whole point of having tests is to allow you to refactor your code!
    Use them, and make your code (including your tests) as clean as you can.
    ((("Test-Driven Development (TDD)", "concepts", "Red/Green/Refactor")))
    ((("Red/Green/Refactor")))

Don't refactor against failing tests::
    * The general rule is that you shouldn't mix refactoring and behaviour
      change. Having green tests is our best guarantee that we aren't changing
      behaviour. If you start refactoring against failing tests, it becomes much
      harder to spot when you're accidentally introducing a regression.
    * This applies strongly to unit tests. With functional tests, because we
      often develop against red FTs anyway, it's sometimes more tempting to
      refactor against failing tests. My suggestion is to avoid that temptation
      and use an early return, so that it's 100% clear if, during a refactory,
      you accidentally introduce a regression that's picked up in your FTs.
    * You can occasionally put a skip on a test which is testing something you
      haven't written yet.
    * More commonly, make a note of the refactor you want to do, finish what
      you're working on, and do the refactor a little later, when you're back
      to a working state.
    * Don't forget to remove any skips before you commit your code! You should
      always review your diffs line by line to catch things like this.
      ((("refactoring")))

Try a generic wait_for helper::
    Having specific helper methods that do explicit waits is great, and it
    helps to make your tests readable.  But you'll also often need an ad-hoc
    one-line assertion or Selenium interaction that you'll want to add a wait
    to.  `self.wait_for` does the job well for me, but you might find a slightly
    different pattern works for you.
    ((("generic explicit wait helper")))
    ((("wait_for helper method")))
    ((("self.wait_for helper method")))

*******************************************************************************
