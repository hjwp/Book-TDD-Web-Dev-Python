[[chapter_09_docker]]
== Containerization aka Docker

[quote, Malvina Reynolds]
______________________________________________________________
Little boxes, all the same
______________________________________________________________

In this chapter, we'll start by adapting our FTs so that they can run against a container.
And then we'll set about containerising our app,
and getting those tests passing our code running inside Docker:

* We'll build a minimal Dockerfile with everything we need to run our site.

* We'll learn how to build and run a container on our machine.

* We'll make a few changes to our source code layout, like using an _src_ folder.

* We'll start flushing out a few issues around networking and the database.

=== Docker, Containers, and Virtualization

Docker is a commercial product that wraps several free
and open source technologies from the world of Linux,
sometimes referred to as "containerization".((("Docker")))((("containerization")))

NOTE: Feel free to skip this section if you already know all about Docker.

You may have already heard of the idea of "virtualization",
which enables a single physical computer to pretend to be several machines.((("virtualization")))
Pioneered by IBM (amongst others) on mainframes in the 1960s,
it rose to mainstream adoption in the '90s,
where it was sold as a way to optimise resource usage in datacentres.
AWS, for example, an offshoot of Amazon,
was using virtualization already,
and realised it could sell some spare capacity on its servers
to customers outside the business.((("Amazon Web Services (AWS)")))((("AWS (Amazon Web Services)")))

So, when you come to deploy your code to a real server in a datacentre,
it will be using virtualization.
And, actually, you can use virtualization on your own machine,
with software like VirtualBox or KVM.
You can run Windows "inside" a Mac or Linux laptop, for example.

But it can be fiddly to set up!((("virtualization", "containerization and")))
And nowadays, thanks to containerization, we can do better
because containerization is a kind of even-more-virtual virtualization.((("VMs (virtual machines)")))

Conceptually, "regular" virtualization works at the hardware level:
it gives you multiple virtual machines (VMs)
that pretend to be different physical computers, on a single real machine.
So you can run multiple operating systems using separate VMs
on the same physical box, as in <<virtualization-diagram>>.

[[virtualization-diagram]]
.Physical versus virtual machines
image::images/tdd3_0901.png["A diagram showing a physical machine, with an operating system and a Python virtualenv running inside it, vs multiple virtual machines running different operating systems on a single real machine"]


// TODO; remove virtualenvs from this diagram, they just confuse things.
// add another diagram later to contrast venvs with dockers.

Containerization works at the operating system (OS) level:
it gives you multiple virtual operating systems that
all run on a single real OS.footnote:[
It's more accurate to say that containers share the same kernel as the host OS.((("operating systems (OS)", "containerization at OS level")))
An operating system is made up of a kernel,
and a bunch of utility programs that run on top of it.
The kernel is the core of the OS;
it's the program that runs all the other programs.
Whenever your program needs to interact with the outside world,
read a file, or talk to the internet, or start another program,
it actually asks the kernel to do it.
Starting about 15 years ago, the Linux kernel grew the ability
to show different filesystems to different programs,
as well as isolate them into different network and process namespaces;
these are the capabilities that underpin Docker and containerization.]

Containers let us pack the source code((("containers"))) and the system dependencies
(like Python or system libraries) together,
and our programs run inside separate virtual systems,
using a single real host OS and kernel.footnote:[
Because containers all share the same kernel,
while virtualization can let you run Windows and Linux on the same machine,
containers on Linux hosts all run Linux, and ones on Windows hosts all run Windows.
If you're running Linux containers on a Mac or a PC,
it's because you're actually running them on a Linux VM under the hood.]
See <<containers-diagram>> for an illustration.

The upshot of this is that containers are much "cheaper".
You can start one up in milliseconds,
and you can run hundreds on the same machine.

NOTE: If you're new to all this, I know it's a lot to wrap your head around!
  It takes a while to build a good mental model of what's happening.((("Docker", "resources on containers")))((("containers", "Docker resources on")))
  Have a look at
  https://www.docker.com/resources/what-container/[Docker's resources on containers]
  for more explanation.
  Hopefully, following along with these chapters and seeing them working in practice
  will help you to better understand the theory.

[[containers-diagram]]
.Containers share a kernel in the host operating system
image::images/tdd3_0902.png["Diagram showing one or more containers running on a single host operating system, showing that each container uses the kernel from the host OS, but is able to have its own filesystem, based on an image, but also possibly mounting directories from the host filesystem"]


==== Why Not Just Use a Virtualenv?

You might be thinking that this sounds a lot like a virtualenvâ€”and you'd be right!
Virtualenvs already let us run different versions of Python,
with different Python packages, on the same machine.((("virtualenv (virtual environment)", "capabilities of Docker and containers versus")))((("Docker", "capabilities of containers versus virtualenvs")))((("containers", "capabilities of versus virtualenvs")))

What Docker containers give us over and above virtualenvs,
is the ability to have different _system_ dependencies too;
things you can't `pip install`, in other words.
In the Python world, this could be C libraries,
like `libpq` for PostgreSQL, or `libxml2` for parsing XML.
But you could also run totally different programming languages
in different containers, or even different Linux distributions.
So, server administrators or platform people like them
because it's one system for running any kind of software,
and they don't need to understand the intricacies of any particular
language's packaging systems.




==== Docker and Your CV

That's all well and good for the _theoretical_ justification,
but let's get to the _real_ reason for using this technology,
which, as always, is:
"it's fashionable so it's going to look good on my CV".((("Docker", "use of, asset on your CV")))((("CV (curriculum vitae), Docker and")))

For the purposes of this book,
that's not such a bad justification really!

Yes, it's going to be a nice way to have a "pretend"
deployment on our own machine, before we try the real one--but
also, containers are so popular nowadays,
that it's very likely that you're going to encounter them at work
(if you haven't already).
For many working developers, a container image is the final artifact of their work;
it's what they "deliver",
and often the rest of the deployment process is something they rarely have to think about.

In any case, without further ado, let's get into it!



=== As Always, Start with a Test

((("environment variables")))((("LiveServerTestCase")))
Let's adapt our functional tests (FTs)
so that they can run against a standalone server,
instead of the one that `LiveServerTestCase` creates for us.

Do you remember I said that `LiveServerTestCase` had certain limitations?
Well, one is that it always assumes you want to use its own test server,
which it makes available at `self.live_server_url`.
I still want to be able to do that _sometimes_,
but I also want to be able to selectively tell it not to bother,
and to use a real server instead.((("TEST_SERVER environment variable")))

We'll do it by checking for an environment variable
called `TEST_SERVER`:

//IDEA; the word "server" is overloaded.
// here we mean docker containers, later we mean a real server.  TEST_HOST??


[role="sourcecode"]
.functional_tests/tests.py (ch09l001)
====
[source,python]
----
import os
[...]

class NewVisitorTest(StaticLiveServerTestCase):
    def setUp(self):
        self.browser = webdriver.Firefox()
        if test_server := os.environ.get("TEST_SERVER"):  # <1><2>
            self.live_server_url = "http://" + test_server  # <3>
----
====


<1> Here's where we check for the env var.((("walrus operator (:&#x3D;)")))(((":&#x3D; (walrus) operator")))

<2> If you haven't seen this before, the `:=` is known as the "walrus operator"
    (more formally, it's the operator for an "assignment expression"),
    which was a controversial new feature from Python 3.8footnote:[
    The feature was a favourite of Guido van Rossum's,
    but the discussion around it was so toxic that Guido
    stepped down from his role as Python's BDFL, or "Benevolent Dictator for Life"]
    and it's not often useful, but it is quite neat for cases like this,
    where you have a variable and want to do a conditional on it straight away.
    See https://www.pythonmorsels.com/using-walrus-operator/[this article]
    for more explanation.

<3> Here's the hack: we replace `self.live_server_url` with the address of
    our "real" server.


NOTE: A clarification: when we say we run tests _against_ our Docker container,
  or _against_ our staging server,
  that doesn't mean we run the tests _from_ Docker or _from_ our staging server.((("Test-Driven Development (TDD)", "concepts", "running tests against")))
  We still run the tests from our own laptop,
  but they target the place that's running our code.


We test that said hack hasn't broken anything by running the FTs [keep-together]#"normally"#:

[subs="specialcharacters,macros"]
----
$ pass:quotes[*python manage.py test functional_tests*]
[...]
Ran 3 tests in 8.544s

OK
----

And now we can try them against our Docker server URLâ€”which, once we've done the right Docker magic,
will be at _http&#58;//localhost:8888_.

TIP: I'm deliberately choosing a different port to run Dockerised Django on (8888)
    from the default port that a local `manage.py runserver` would choose (8080).
    This is to avoid getting in the situation where I (or the tests) _think_
    we're looking at Docker, when we're actually looking at a local `runserver`
    that I've left running in some terminal somewhere.((("Django framework", "running Dockerized Django")))

.Ports
*******************************************************************************
Ports are what let you have multiple connections open at the same time on a single machine,
the reason you can load two different websites at the same time, for example.((("ports")))((("network adapters, range of ports")))

Each network adapter has a range of ports, numbered from 0 to 65535.
In a client/server connection, the client knows the port of the server,
and the client OS chooses a random local port for its side of the connection.

When a server is "listening" on a port,
no other service can bind to that port at the same time.
That's why you can't run `manage.py runserver` in two different terminals
at the same time, because both want to use port 8080 by default.
*******************************************************************************

We'll use the `--failfast` option to exit as soon as((("--failfast option", primary-sortas="failfast"))) a single test fails:


[role="small-code"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*TEST_SERVER=localhost:8888 ./manage.py test functional_tests --failfast*]
[...]
E
======================================================================
ERROR: test_can_start_a_todo_list
(functional_tests.tests.NewVisitorTest.test_can_start_a_todo_list)
 ---------------------------------------------------------------------
Traceback (most recent call last):
  File "...goat-book/functional_tests/tests.py", line 38, in
test_can_start_a_todo_list
    self.browser.get(self.live_server_url)
[...]

selenium.common.exceptions.WebDriverException: Message: Reached error page: abo
ut:neterror?e=connectionFailure&u=http%3A//localhost%3A8888/[...]


Ran 1 tests in 5.518s

FAILED (errors=1)
----

NOTE: If, on Windows, you see an error saying something like
    "TEST_SERVER is not recognized as a command",
  it's probably because you're not using Git Bash.((("Git Bash")))((("Bash shell (Git Bash)")))
  Take another look at the <<pre-requisites>> section.

You can see that our tests are failing, as expected, because we're not running Docker yet.
Selenium reports that Firefox is seeing an error and "cannot establish connection to the server",
and you can see _localhost:8888_ in there too.


The FT seems to be testing the right things, so let's commit:

[subs="specialcharacters,quotes"]
----
$ *git diff* # should show changes to functional_tests.py
$ *git commit -am "Hack FT runner to be able to test docker"*
----


TIP: Don't use `export` to set the 'TEST_SERVER' environment variable;
    otherwise, all your subsequent test runs in that terminal will be against staging,
    and that can be very confusing if you're not expecting it.
    Setting it explicitly inline each time you run the FTs is best.


==== Making a src Folder

When preparing a codebase for deployment,
it's often convenient to separate out the actual source code of our production app
from the rest of the files that you need in the project.
A folder called _src_ is a common convention.((("src folder")))

Currently, all our code is source code really, so we move everything into _src_
(we'll be seeing some new files appearing outside _src_ shortly):footnote:[
A common thing to find outside of the _src_ folder is a folder called _tests_.
We won't be doing that while we're relying on the standard Django test framework,
but it can be a good thing to do if you're using pytest, for example.]



//002
[subs="specialcharacters,quotes"]
----
$ *mkdir src*
$ *git mv functional_tests lists superlists manage.py src*
$ *git commit -m "Move all our code into a src folder"*
----


=== Installing Docker

The https://docs.docker.com/get-docker[Docker documentation] is pretty good,
and you'll find detailed installation instructions for Windows, Mac, and Linux.((("Docker", "installing")))

TIP: Choose WSL (Windows Subsystem for Linux) as your backend on Windows,
    as we'll need it in the next chapter.((("WSL (Windows Subsystem for Linux)")))((("Windows Subsystem for Linux (WSL)")))
    You can find installation instructions
    https://learn.microsoft.com/en-us/windows/wsl/install[on the Microsoft website].
    This doesn't mean you have to switch your development environment
    to being "inside" WSL; Docker just uses WSL as a virtualisation engine
    in the background.
    You should be able to run all the `docker` CLI commands from the
    same Git Bash console you've been using so far.


// TODO: appendix or link to more detailed instructions for WSL use?



[[docker-alternatives]]
.Docker Alternatives: Podman, nerdctl, etc
*****************************************************************************************
Impartiality commands me to also mention
https://podman.io[Podman] and
https://github.com/containerd/nerdctl[nerdctl],
both like-for-like replacements for Docker.

They are both((("Docker", "alternatives to")))((("Podman")))((("nerdctl"))) pretty much exactly the same as Docker,
arguably with a few advantages even.footnote:[
Docker uses a central "daemon" to manage containers,
which Podman and nerdctl don't.]

I actually tried Podman out on early drafts of this chapter (on Linux)
and it worked perfectly well.
But they are both a little less well established and documented;
the Windows installation instructions are a little more DIY, for example.
So in the end, although I'm always a fan of a plucky noncommercial upstart,
I decided to stick with Docker for now.  After all,
the core of it is still open source, to its credit!
But you could definitely check out one of the alternatives if you feel like it.

You can follow along all the instructions in the book
by just substituting the `docker` binary for `podman` or `nerdctl`
in all the CLI instructions:

[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *docker run busybox echo hello*
# becomes
$ *podman run busybox echo hello*
# or
$ *nerdctl run busybox echo hello*
# similarly with podman build, nerdtcl build, podman ps, etc.
----


*****************************************************************************************

.Colima: An Alternative Docker Runtime for macOS
*****************************************************************************************
If you're on macOS,
you might find the Docker Dekstop licensing terms don't work for you.((("Colima, alternative container runtime for MacOS")))
In that case, you can try https://github.com/abiosoft/colima[Colima],
which is a "container runtime", essentially the backend for Docker.
You still use the Docker CLI tools,
but Colima provides the server to run the containers:

[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *docker run busybox echo hello*
docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is
the docker daemon running?.
See 'docker run --help'.
$ *colima start*
INFO[0001] starting colima
INFO[0001] runtime: docker
INFO[0001] starting ...                                  context=vm
INFO[0014] provisioning ...                              context=docker
INFO[0016] starting ...                                  context=docker
INFO[0017] done
$ *docker run busybox echo hello*
hello
----

I used Colima for most of the writing of this book,
and it worked fine for me.((("DOCKER_HOST environment variable")))
The only thing I needed to do was set the `DOCKER_HOST` environment variable,
and that only came up in <<chapter_12_ansible>>:

[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *export DOCKER_HOST=unix:///$HOME/.colima/default/docker.sock
----

NOTE: On macOS, you can use Colima as a backend for nerdctl.
  Podman ships with its own runtime, for both Mac and Windows
  (there is no need for a runtime on Linux).

At the time of writing, Apple had just announced its own container runner,
https://github.com/apple/container[container],
but it was in beta and I didn't have time to try it out.
*****************************************************************************************

Test your installation by running:

[subs="specialcharacters,macros"]
----
$ pass:quotes[*docker run busybox echo hello world*]
Unable to find image 'busybox:latest' locally
[...]
latest: Pulling from library/busybox
[...]: Pull complete
Digest: sha256:[...]
Status: Downloaded newer image for busybox:latest
hello world
----

What's happened there is that Docker has:

* Searched for a local copy of the "busybox" image and not found it
* Downloaded the image from Docker Hub
* Created a container based on that image
* Started up that container, telling it to run `echo hello world`
* And we can see it worked!

Cool! We'll find out more about all of these steps as the chapter progresses.


NOTE: On macOS, if you get errors saying `command not found: docker"`,
  obviously the first thing you should do is Google for "macOS command not found Docker",
  but at least one reader has reported that the solution was
  Docker Desktop > Settings > Advanced > Change from User to System.


=== Building a Docker Image and Running a Docker Container

Docker has the concepts of _images_ as well as containers.
An image is essentially a pre-prepared root filesystem,
including the OS, dependencies, and any code you want to run.((("images (container)")))((("Docker", "building image and running a container", id="ix_Dckimg")))

Once you have an image, you can run one or more containers that use the same image.
It's a bit like saying, once you've installed your OS and software,
you can start up your computer and run that software any number of times,
without needing to change anything else.

Another way of thinking about it is: images are like classes,
and containers are like instances.


==== A First Cut of a Dockerfile

Think of a Dockerfile as instructions for setting up a brand new computer
that we're going to use to run our Django server on.((("Docker", "building image and running a container", "first draft of Dockerfile")))((("Dockerfiles")))
What do we need to do?  Something like this, right?

1. Install an operating system.
2. Make sure it has Python on it.
3. Get our source code onto it.
4. Run `python manage.py runserver`.


We create a new file called _Dockerfile_ in the base folder of our repo,
next to the _src/_ directory we made earlier:


[role="sourcecode"]
.Dockerfile (ch09l003)
====
[source,dockerfile]
----
FROM python:3.13-slim  # <1>

COPY src /src  # <2>

WORKDIR /src  # <3>

CMD ["python", "manage.py", "runserver"]  # <4>
----
====

<1> The `FROM` line is usually the first thing in a Dockerfile,
    and it says which _base image_ we are starting from.
    Docker images are built from other Docker images!
    It's not quite turtles all the way down, but almost.
    So this is the equivalent of choosing a base OS,
    but images can actually have lots of software preinstalled too.
    You can browse various base images on Docker Hub.
    We're using https://hub.docker.com/_/python[one that's published by the Python Software Foundation],
    called "slim" because it's as small as possible.
    It's based on a popular version of Linux called Debian,
    and of course it comes with Python already installed on it.

<2> The `COPY` instruction (the uppercase words are called "instructions")
    lets you copy files from your own computer into the container image.
    We use it to copy all our source code from the newly-created _src_ folder,
    into a similarly named folder at the root of the container image.

<3> `WORKDIR` sets the current working directory for all subsequent commands.
     It's a bit like doing `cd /src`.

<4> Finally, the `CMD` instruction tells Docker which command you want it to run
    by default, when you start a container based on that image.
    The syntax is a bit like a Python list
    (although it's actually parsed as a JSON array, so you _have_ to use double quotes).


It's probably worth just showing a directory tree,
to make sure everything is in the right place, right?
All our source code is in a folder called _src_,
next to our `Dockerfile`:

[[tree-with-src-and-dockerfile]]
[subs="specialcharacters,macros"]
----
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ db.sqlite3
â”œâ”€â”€ src
â”‚Â Â  â”œâ”€â”€ functional_tests
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [...]
â”‚Â Â  â”œâ”€â”€ lists
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ [...]
â”‚Â Â  â”œâ”€â”€ manage.py
â”‚Â Â  â””â”€â”€ superlists
â”‚Â Â      â”œâ”€â”€ [...]
â””â”€â”€ static
    â””â”€â”€ [...]
----

// TODO: figure out what to do with the /static folder


==== Docker Build

You build an image with `docker build <path-containing-dockerfile>`
and we'll use the `-t <tagname>` argument to "tag" our image
with a memorable name.((("Docker", "building image and running a container", "docker build command")))

It's typical to invoke `docker build` from the folder that contains your Dockerfile,
so the last argument is usually `.`:

[subs="specialcharacters,macros"]
----
$ pass:quotes[*docker build -t superlists .*]
[+] Building 1.2s (8/8) FINISHED                            docker:default
 => [internal] load build definition from Dockerfile                  0.0s
 => => transferring dockerfile: 115B                                  0.0s
 => [internal] load .dockerignore                                     0.1s
 => => transferring context: 2B                                       0.0s
 => [internal] load metadata for docker.io/library/python:slim        3.4s
 => [internal] load build context                                     0.2s
 => => transferring context: 68.54kB                                  0.1s
 => [1/3] FROM docker.io/library/python:3.13-slim@sha256:858[...]     4.4s
 => => resolve docker.io/library/python:3.13-slim@sha256:858[...]     0.0s
 => => sha256:72ba3400286b233f3cce28e35841ed58c9e775d69cf11f[...]     0.0s
 => => sha256:3a72e7f66e827fbb943c494df71d2ae024d0b1db543bf6[...]     0.0s
 => => sha256:a7d9a0ac6293889b2e134861072f9099a06d78ca983d71[...]     0.5s
 => => sha256:426290db15737ca92fe1ee6ff4f450dd43dfc093e92804[...]     4.0s
 => => sha256:e8b685ab0b21e0c114aa94b28237721d66087c2bb53932[...]     0.5s
 => => sha256:85824326bc4ae27a1abb5bc0dd9e08847aa5fe73d8afb5[...]     0.0s
 => => extracting sha256:a7d9a0ac6293889b2e134861072f9099a06[...]     0.1s
 => => extracting sha256:426290db15737ca92fe1ee6ff4f450dd43d[...]     0.4s
 => => extracting sha256:e8b685ab0b21e0c114aa94b28237721d660[...]     0.0s
 => [internal] load build context                                     0.0s
 => => transferring context: 7.56kB                                   0.0s
 => [2/3] COPY src /src                                               0.2
 => [3/3] WORKDIR /src                                                0.1s
 => exporting to image                                                0.0s
 => => exporting layers                                               0.0s
 => => writing image sha256:7b8e1c9fa68e7bad7994fa41e2aca852ca79f01a  0.0s
 => => naming to docker.io/library/superlists                         0.0s
----

Now we can see our image in the list of Docker images on the system:

// IDEA, this listing was hard to test due to column widths but there must be a way

[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *docker images*
REPOSITORY   TAG       IMAGE ID       CREATED          SIZE
superlists   latest    522824a399de   2 minutes ago    164MB
[...]
----



NOTE: If you see an error about `failed to solve / compute cache key` and `src: not found`,
  it may be because you saved the Dockerfile in the wrong place.
  Have another look at the directory tree from earlier <<tree-with-src-and-dockerfile>>.



==== Docker Run

Once you've built an image,
you can run one or more containers based on that image, using `docker run`.
What happens when we run ours?((("Docker", "building image and running a container", "docker run command")))


[role="ignore-errors"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*docker run superlists*]
Traceback (most recent call last):
  File "/src/manage.py", line 11, in main
    from django.core.management import execute_from_command_line
ModuleNotFoundError: No module named 'django'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/src/manage.py", line 22, in <module>
    main()
    ~~~~^^
  File "/src/manage.py", line 13, in main
    raise ImportError(
    ...<3 lines>...
    ) from exc
ImportError: Couldn't import Django. Are you sure it's installed and available
on your PYTHONPATH environment variable? Did you forget to activate a virtual
environment?
----


Ah, we forgot that we need to install Django.((("Docker", "building image and running a container", startref="ix_Dckimg")))


=== Installing Django in a Virtualenv in our Container Image

Just like on our own machine,
a virtualenv((("Django framework", "installing in virtualenv in container image", id="ix_Djainctnrimg")))((("containers", "installing Django in virtualenv in container image", id="ix_cntnrDja")))((("virtualenv (virtual environment)", "installing Django in virtualenv in container image"))) is useful in a deployed environment to make
sure we have full control over the packages installed
for a particular project.footnote:[
Even a completely fresh Linux install might have odd things installed
in its system site packages.
A virtualenv is a guaranteed clean slate.]

We can create a virtualenv in our Dockerfile
just like we did on our own machine with `python -m venv`,
and then we can use `pip install` to get Django:


[role="sourcecode"]
.Dockerfile (ch09l004)
====
[source,dockerfile]
----
FROM python:3.13-slim

RUN python -m venv /venv  <1>
ENV PATH="/venv/bin:$PATH"  <2>

RUN pip install "django<6" <3>

COPY src /src

WORKDIR /src

CMD ["python", "manage.py", "runserver"]
----
====

<1> Here's where we create our virtualenv.
    We use the `RUN` Dockerfile directive,
    which is how you run arbitrary shell commands as part of
    building your Docker image.

<2> You can't really "activate" a virtualenv inside a Dockerfile,
    so instead we change the system path so that the venv versions
    of `pip` and `python` become the default ones
    (this is actually one of the things that `activate` does, under the hood).

<3> We install Django with `pip install`, just like we do locally.



==== Successful Run

Let's do the `build` and `run` in a single line.
This is a pattern I used quite often when developing a Dockerfile,
to be able to quickly rebuild and see the effect of a change:

[role="small-code"]
[subs="specialcharacters,quotes"]
----
$ *docker build -t superlists . && docker run -it superlists*
[+] Building 0.2s (11/11) FINISHED                                  docker:default
[...]
 => [internal] load .dockerignore                                   0.1s
 => => transferring context: 2B                                     0.0s
 => [internal] load build definition from Dockerfile                0.0s
 => => transferring dockerfile: 246B                                0.0s
 => [internal] load metadata for docker.io/library/python:slim      0.0s
 => CACHED [1/5] FROM docker.io/library/python:slim                 0.0s
 => [internal] load build context                                   0.0s
 => => transferring context: 4.75kB                                 0.0s
 => [2/5] RUN python -m venv /venv                                  0.0s
 => [3/5] pip install "django<6"                                    0.0s
 => [4/5] COPY src /src                                             0.0s
 => [5/5] WORKDIR /src                                              0.0s
 => exporting to image                                              0.0s
 => => exporting layers                                             0.0s
 => => writing image sha256:[...]                                   0.0s
 => => naming to docker.io/library/superlists                       0.0s
Watching for file changes with StatReloader
Performing system checks...

System check identified no issues (0 silenced).

You have 19 unapplied migration(s). Your project may not [...]
[...]
Django version 5.2, using settings 'superlists.settings'
Starting development server at http://127.0.0.1:8000/
Quit the server with CONTROL-C.
----


OK, scanning through that, it looks like the server is running!


WARNING: Make sure you use the `-it` flags to the Docker `run`
    command when running `runserver`, or any other tool that expects
    to be run in an interactive terminal session,
    otherwise you'll get strange behaviour, including not being able
    to interrupt the Docker process with Ctrl+C.
    See the following sidebar for an escape hatch.


[[how-to-stop-a-docker-container]]
.How to Stop a Docker Container
*******************************************************************************
If you've got a container that's "hanging" in a terminal window,
you can stop it from another terminal.

The Docker daemon lets you list all the currently running containers
with `docker ps`:

[role="skipme small-code"]
[subs="quotes"]
----
$ *docker ps*
CONTAINER ID   IMAGE        COMMAND                  CREATED         STATUS         PORTS     NAMES
0818e1b8e9bf   superlists   "/bin/sh -c 'python â€¦"   4 seconds ago   Up 4 seconds             hardcore_moore
----

This tells us a bit about each container, including a unique ID
and a randomly-generated name (you can override that if you want to).

We can use the ID or the name to terminate the container with `docker stop`:footnote:[
There is also a `docker kill` if you're in a hurry.
But `docker stop` will send a `SIGKILL` if its initial `SIGTERM`
doesn't work within a certain timeout (more info in
https://docs.docker.com/reference/cli/docker/container/stop[the Docker docs]).]

[role="skipme"]
[subs="quotes"]
----
$ *docker stop 0818e1b8e9bf*
0818e1b8e9bf
----

And if you go back to your other terminal window,
you should find that the Docker process has been terminated.((("containers", "installing Django in virtualenv in container image", startref="ix_cntnrDja")))((("Django framework", "installing in virtualenv in container image", startref="ix_Djainctnrimg")))

*******************************************************************************



=== Using the FT to Check That Our Container Works

Let's see what our FTs think about ((("containers", "checking that Docker container works")))this Docker version of our site:


[role="small-code"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*TEST_SERVER=localhost:8888 ./src/manage.py test src/functional_tests --failfast*]
[...]
selenium.common.exceptions.WebDriverException: Message: Reached error page: abo
ut:neterror?e=connectionFailure&u=http%3A//localhost%3A8888/[...]
----

What's going on here?  Time for a little debugging.



=== Debugging Container Networking Problems

First, let's try and take a look ourselves, in our browser, by((("debugging", "of container networking problems", secondary-sortas="container")))((("containers", "debugging networking problems for"))) going to http://localhost:8888/:

[[firefox-unable-to-connect-screenshot]]
.Cannot connect on that port
image::images/tdd3_0903.png["Firefox showing the 'Unable to connect' error"]

Now, let's take another look at the output from our `docker run`.
Here's what appeared right at the end:


[role="skipme"]
----
Starting development server at http://127.0.0.1:8000/
Quit the server with CONTROL-C.
----

Aha!  We notice that we're using the wrong port, the default `8000` instead of the `8888`
that we specified in the `TEST_SERVER` environment variable (or, "env var").

Let's fix that by amending the `CMD` instruction in the Dockerfile:


[role="sourcecode"]
.Dockerfile (ch09l005)
====
[source,dockerfile]
----
[...]
WORKDIR /src

CMD ["python", "manage.py", "runserver", "8888"]
----
====

Ctrl+C the current Dockerized container process if it's still running in your terminal,
then give it another `build && run`:

[subs="specialcharacters,quotes"]
----
$ *docker build -t superlists . && docker run -it superlists*
[...]
Starting development server at http://127.0.0.1:8888/
----


==== Debugging Web Server Connectivity With curl

A quick run of the FT or check in our browser will show us that nope, that doesn't work either.((("debugging", "of web server connectivity using curl", secondary-sortas="web")))((("curl utility")))
Let's try an even lower-level smoke test, the traditional Unix utility `curl`.
It's a command-line tool for making HTTP requests.footnote:[
`curl` can do FTP (File Transfer Protocol) and many other types of network requests too! Check out the https://man7.org/linux/man-pages/man1/curl.1.html[`curl` manual].]
Try it on your own computer first:

[role="ignore-errors"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*curl -iv localhost:8888*]
*   Trying 127.0.0.1:8888...
* connect to 127.0.0.1 port 8888 [...]
*   Trying [::1]:8888...
* connect to ::1 port 8888 [...]
* Failed to connect to localhost port 8888 after 0 ms: [...]
* Closing connection
[...]
curl: (7) Failed to connect to localhost port 8888 after 0 ms: [...]
----

TIP: The `-iv` flag to `curl` is useful for debugging.
    It prints verbose output, as well as full HTTP headers.



=== Running Code "Inside" the Container with docker exec

So, we can't see Django running on port `8888` when we're _outside_ the container.
What do we see if we run things from _inside_ the container?((("containers", "running code inside with docker exec", id="ix_cntnrrun")))((("Docker", "running code inside container with docker exec", id="ix_Dckexec")))

We can use `docker exec` to run commands inside a running container.
First, we need to get the name or ID of the container:

// TODO use --name arg to docker run??

[role="skipme small-code"]
[subs="specialcharacters,quotes"]
----
$ *docker ps*
CONTAINER ID   IMAGE        COMMAND                  CREATED          STATUS          PORTS     NAMES
5ed84681fdf8   superlists   "/bin/sh -c 'python â€¦"   12 minutes ago   Up 12 minutes             trusting_wu
----

Your values for `CONTAINER_ID` and `NAMES` will be different from mine,
because they're randomly generated.
But make a note of one or the other, and then run `docker exec -it <container-id> bash`.
On most platforms, you can use tab completion for the container ID or name.

Let's try it now.  Notice that the shell prompt will change from your default Bash prompt
to `root@container-id`.  Watch out for those in future listings,
so that you can be sure of what's being run inside versus outside containers.

[role="skipme"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*docker exec -it container-id-or-name bash*]
root@5ed84681fdf8:/src# pass:specialcharacters,quotes[*apt-get update && apt-get install -y curl*]
Get:1 pass:[http://deb.debian.org/debian] bookworm InRelease [151 kB]
Get:2 pass:[http://deb.debian.org/debian] bookworm-updates InRelease [52.1 kB]
[...]
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following additional packages will be installed:
  libbrotli1 libcurl4 libldap-2.5-0 libldap-common libnghttp2-14 libpsl5
[...]
root@5ed84681fdf8:/src# pass:quotes[*curl -iv http://localhost:8888*]
*   Trying [...]
* Connected to localhost [...]
> GET / HTTP/1.1
> Host: localhost:8888
> User-Agent: curl/8.6.0
> Accept: */*
>
< HTTP/1.1 200 OK
HTTP/1.1 200 OK
[...]
<!doctype html>
<html lang="en">

  <head>
    <title>To-Do lists</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="/static/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  </head>

  <body>
    [...]
  </body>

</html>
----

TIP:  Use Ctrl+D to exit from the `docker exec` bash shell inside the container.

That's definitely some HTML! And the `<title>To-Do lists</title>` looks like it's our HTML, too.

So, we can see Django is serving our site _inside_ the container. Why can't we see it _outside_?

==== Docker Port Mapping

The (highly, highly recommend) PythonSpeed guide to Docker's very first section is called
https://pythonspeed.com/articles/docker-connection-refused[Connection refused?],
so I'll refer you there once again for an _excellent_, detailed explanation.((("ports", "Docker port mapping")))

But in short: Docker runs in its own little world;
specifically, it has its own little network,
so the ports _inside_ the container are different
from the ports _outside_ the container, the ones we can see on our host machine.

So, we need to tell Docker to connect the internal ports to the outside onesâ€”to "publish" or "map" them, in Docker terminology.

`docker run` takes a `-p` argument, with the syntax `OUTSIDE:INSIDE`.
So, you can actually map a different port number on the inside and outside.
But we're just mapping `8888` to `8888`, and that will look like this:

[subs="specialcharacters,quotes"]
----
$ *docker build -t superlists . && docker run -p 8888:8888 -it superlists*
----

Now that will _change_ the error we see, but only quite subtly (see <<firefox-connection-reset>>).footnote:[
Tip: If you use Chrome as your web browser,
its error is something like "localhost didnâ€™t send any data. ERR_EMPTY_RESPONSE".]
Things clearly aren't working yet.


[[firefox-connection-reset]]
.Cannot connect on that port
image::images/tdd3_0904.png["Firefox showing the 'Connection reset' error"]

// FT would show this
// selenium.common.exceptions.WebDriverException: Message: Reached error page: about:neterror?e=netReset&u=http%3A//localhost%3A8888/&c=UTF-8&d=The%20connection%20to%20the%20server%20was%20reset%20while%20the%20page%20was%20loading.

Similarly, if you try our `curl -iv` (outside the container) once again,
you'll see the error has changed from "Failed to connect",
to "Empty reply":

// CI consistently says "connection reset by peer",
// locally it's empty reply, no matter what curl version

[role="ignore-errors skipme"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*curl -iv localhost:8888*]
*   Trying [...]
* Connected to localhost (127.0.0.1) port 8888
> GET / HTTP/1.1
> Host: localhost:8888
> User-Agent: curl/8.6.0
> Accept: */*
[...]
* Empty reply from server
* Closing connection
curl: (52) Empty reply from server
----

NOTE: Depending on your system, instead of `(52) Empty reply from server`,
  you might see `(56) Recv failure: Connection reset by peer`.
  They mean the same thing: we can connect but we don't get a response.


==== Essential Googling the Error Message

The need to map ports and the `-p` argument to `docker run` are something you just pick up,
fairly early on in learning Docker.((("error messages", "Django runserver inside Docker, access problem")))((("&quot;Googling the error message&quot; technique", primary-sortas="Googling")))
But the next debugging step is quite a bit more obscureâ€”although admittedly Itamar does address it in his
https://pythonspeed.com/articles/docker-connection-refused[Docker networking article] (did I already mention how excellent it is?).


But if we haven't read that, we can always resort to the tried and tested
"Googling the error message" technique instead (<<googling-the-error>>).


[[googling-the-error]]
.An indispensable publication (source: https://news.ycombinator.com/item?id=11459601[])
image::images/tdd3_0905.png["Cover of a fake O'Reilly book called Essential Googling the Error Message",400]

Everyone's search results are a little different,
and mine are perhaps shaped by years of working with Docker and Django,
but I found the answer in my very first result
(see <<google-results-screenshot>>),
when I searched for "cannot access Django runserver inside Docker".
The result was was a https://stackoverflow.com/questions/49476217/docker-cant-access-django-server[Stack Overflow post],
saying something about needing to specify `0.0.0.0` as the IP address.footnote:[
Kids these days will probably ask an AI right?
I have to say, I tried it out, with the prompt being
"I'm trying to run Django inside a Docker container,
and I've mapped port 8888, but I still can't connect.
Can you suggest what the problem might be?",
and it come up with a pretty good answer.]


[[google-results-screenshot]]
.Google can still deliver results
image::images/tdd3_0906.png["Google results with a useful stackoverflow post in first position",1000]


We're nearing the edges of my understanding of Docker now,
but as I understand it, `runserver` binds to `127.0.0.1` by default.
However, that IP address doesn't correspond to a network adapter _inside_
the container, which is actually connected to the outside world
via the port mapping we defined earlier.

The long and short of it is that
we need use the long-form `ipaddr:port` version of the `runserver` command,
using the magic "wildcard" IP address, `0.0.0.0`:


[role="sourcecode"]
.Dockerfile (ch09l007)
====
[source,dockerfile]
----
[...]
WORKDIR /src

CMD ["python", "manage.py", "runserver", "0.0.0.0:8888"]
----
====

Rebuild and rerun your server, and if you have eagle eyes,
you'll spot it's binding to `0.0.0.0` instead of `127.0.0.1`:

[subs="specialcharacters,quotes"]
----
$ *docker build -t superlists . && docker run -p 8888:8888 -it superlists*
[...]
Starting development server at http://0.0.0.0:8888/
----


We can verify it's working with `curl`:

[subs="specialcharacters,macros"]
----
$ pass:quotes[*curl -iv localhost:8888*]
*   Trying [...]
* Connected to localhost [...]
[...]

  </body>

</html>
----

Looking good!((("Docker", "running code inside container with docker exec", startref="ix_Dckexec")))((("containers", "running code inside with docker exec", startref="ix_cntnrrun")))


.On Debugging
*******************************************************************************
Let me let you in on a little secret: I'm actually not that good at debugging.
We all have our psychological strengths and weaknesses,
and one of my weaknesses is that
when I run into a problem that I can't see an obvious solution to,
I want to throw up my hands way too soon
and say "well, this is hopeless; it can't be fixed",
and give up.((("debugging", "patience and tenacity in")))

Thankfully I have had some good role models over the years
who are much better at it than me (hi, Glenn!).
Debugging needs the patience and tenacity of a bloodhound.
If at first you don't succeed,
you need to systematically rule out options,
check your assumptions,
eliminate various aspects of the problem, simplify things down, and
find the parts that do and don't work,
until you eventually find the cause.

It might seems hopeless at first! But you usually get there eventually.

*******************************************************************************


=== Database Migrations

((("database migrations", "into Docker container", secondary-sortas="Docker", id="ix_DBmigDck")))((("Docker", "testing database migrations in", id="ix_Dcktstdb")))
A quick visual inspection confirms--the site is up (<<site-in-docker-is-up>>)!

[[site-in-docker-is-up]]
.The site in Docker is up!
image::images/tdd3_0907.png["The front page of the site, at least, is up"]


Let's see what our functional tests say:

[role="small-code"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*TEST_SERVER=localhost:8888 ./src/manage.py test src/functional_tests --failfast*]
[...]
E
======================================================================
ERROR: test_can_start_a_todo_list
(functional_tests.tests.NewVisitorTest.test_can_start_a_todo_list)
 ---------------------------------------------------------------------
Traceback (most recent call last):
  File "...goat-book/src/functional_tests/tests.py", line 56, in
test_can_start_a_todo_list
    self.wait_for_row_in_list_table("1: Buy peacock feathers")
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "...goat-book/src/functional_tests/tests.py", line 26, in
wait_for_row_in_list_table
    table = self.browser.find_element(By.ID, "id_list_table")
[...]
selenium.common.exceptions.NoSuchElementException: Message: Unable to locate
element: [id="id_list_table"]; For documentation [...]
----

Although the FTs can connect happily and interact with our site,
they are failing as soon as they try to submit a new item.

[[django-debug-screen]]
.But the database isn't
image::images/tdd3_0908.png["Django DEBUG page showing database error"]

You might have spotted the yellow Django debug page (<<django-debug-screen>>)
telling us why.
It's because we haven't set up the database
(which, as you may remember, we highlighted as one of the "danger areas" of deployment).


NOTE: The tests saved us from potential embarrassment there.
    The site _looked_ fine when we loaded its front page.
    If we'd been a little hasty and only tested manually,
    we might have thought we were done,
    and it would have been the first users that discovered that nasty Django debug page.
    Okay, slight exaggeration for effectâ€”maybe we _would_ have checked,
    but what happens as the site gets bigger and more complex?
    You can't check everything. The tests can.



To be fair, if you look back through the `runserver` command output
each time we've been starting our container,
you'll see it's been warning us about this issue:


[role="skipme"]
----
You have 19 unapplied migration(s). Your project may not work properly until
you apply the migrations for app(s): auth, contenttypes, lists, sessions.
Run 'python manage.py migrate' to apply them.
----



NOTE: If you don't see this error,
    it's because your _src_ folder had the database file in it, unlike mine.
    For the sake of argument,
    run `rm src/db.sqlite3` and rerun the build and run commands,
    and you should be able to reproduce the error.  I promise it's instructive!


==== Should We Run migrate Inside the Dockerfile? No.

So, should we include `manage.py migrate` in our Dockerfile?

If you try it, you'll find it certainly _seems_ to fix the problem:

[role="sourcecode"]
.Dockerfile (ch09l008)
====
[source,dockerfile]
----
[...]
WORKDIR /src

RUN python manage.py migrate --noinput  <1>

CMD ["python", "manage.py", "runserver", "0.0.0.0:8888"]
----
====

<1> We run `migrate` using the `--noinput` argument to suppress any little "are you sure" prompts.


If we rebuild the image...

[subs="specialcharacters,quotes"]
----
$ *docker build -t superlists . && docker run -p 8888:8888 -it superlists*
[...]
Starting development server at http://0.0.0.0:8888/
----

...and try our FTs again, they all pass!

[role="small-code"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*TEST_SERVER=localhost:8888 ./src/manage.py test src/functional_tests --failfast*]
[...]
...
 ---------------------------------------------------------------------
Ran 3 tests in 26.965s

OK
----

The problem is that this saves our database file into our system image,
which is not what we want,
because the system image is meant to be something fixed and stateless (whereas the database is living, stateful data that should change over time).

[role="pagebreak-before less_space"]
.What Would Happen if We Kept the Database File in the Image
*******************************************************************************
You can try this as a little experiment.
Assuming you've got the `manage.py migrate` line in your Dockerfile:

1. Create a new to-do list and keep a note of its URL (e.g., at http&#58;//localhost:8888/lists/1).
2. Now, `docker stop` your container, and rebuild a new one with the same
  `build && run` command we used earlier.

3. Go back and try to retrieve your old list.  It's gone!

This is because rebuilding the image
will give us a brand new database each time.

What we actually want is for our database storage to be "outside" the container somehow,
so it can persist between different versions of our Docker image.((("database migrations", "into Docker container", secondary-sortas="Docker", startref="ix_DBmigDck")))((("Docker", "testing database migrations in", startref="ix_Dcktstdb")))

*******************************************************************************

=== Mounting Files Inside the Container

We want the database on the server to be totally separate data from the data in
the system image. In most deployments, you'd probably be talking to a separate database server,
like PostgreSQL. For the purposes of this book,
the easiest analogy for a database that's "outside" our container is to access the database from the filesystem outside the container.

That also gives us a convenient excuse to talk about mounting files in Docker,
which is a very Useful Thing to be Able to Do (TM).


First, let's revert our change:

[role="sourcecode"]
.Dockerfile (ch09l009)
====
[source,dockerfile]
----
[...]
COPY src /src

WORKDIR /src

CMD ["python", "manage.py", "runserver", "0.0.0.0:8888"]
----
====


Then, let's make sure we _do_ have the database on our local filesystem,
by running `migrate` (when we moved everything into _./src_, we left the database file behind):

[subs="specialcharacters,quotes"]
----
$ *./src/manage.py migrate --noinput*
Operations to perform:
  Apply all migrations: auth, contenttypes, lists, sessions
Running migrations:
  Applying contenttypes.0001_initial... OK
[...]
  Applying sessions.0001_initial... OK
----

Let's make sure to _.gitignore_ the new location of the database file,
and we'll also use a file called https://docs.docker.com/reference/dockerfile/#dockerignore-file[_.dockerignore_]
to make sure we can't copy our local dev database into our Docker image
during Docker builds:

[subs="specialcharacters,quotes"]
----
$ *echo src/db.sqlite3 >> .gitignore*
$ *echo src/db.sqlite3 >> .dockerignore*
----
//ch09l010, ch09l011

Now we rebuild, and try mounting our database file.
The extra flag to add to the Docker run command is `--mount`,
where we specify `type=bind`, the `source` path on our machine,footnote:[
If you're wondering about the `$PWD` in the listing,
it's a special environment variable that represents the current directory.
The initials echo the `pwd` command, which stands for "print working directory".
Docker requires mount paths to be absolute paths.]
and the `target` path _inside_ the container:

[subs="specialcharacters,quotes"]
----
$ *docker build -t superlists . && docker run \
  -p 8888:8888 \
  --mount type=bind,source="$PWD/src/db.sqlite3",target=/src/db.sqlite3 \
  -it superlists*
----

TIP: You're likely to come across the old syntax for mounts, which was `-v`.
    One of the advantages of the new `--mount` version is that
    it will fail hard if the path you're trying to mount does not existâ€”it says something like `bind source path does not exist`. This avoids a lot of pain (ask me how I know this).


[role="small-code"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*TEST_SERVER=localhost:8888 ./src/manage.py test src/functional_tests --failfast*]
[...]
...
 ---------------------------------------------------------------------
Ran 3 tests in 26.965s

OK
----

AMAZING, IT ACTUALLY WORKSSSSSSSS.

Ahem, that's definitely good enough for now!  Let's commit:


[subs="specialcharacters,quotes"]
----
$ *git add -A .*  # add Dockerfile, .dockerignore, .gitignore
$ *git commit -am"First cut of a Dockerfile"*
----

Phew.  Well, it took a bit of hacking about,
but now we can be reassured that the basic Docker plumbing works.
Notice that the FT was able to guide us incrementally towards a working config,
and spot problems early on (like the missing database).

But we really can't be using the Django dev server in production,
or running on port `8888` forever.
In the next chapter, we'll make our hacky image more production-ready.

But first, time for a well-earned tea break I think, and perhaps a
https://en.wikipedia.org/wiki/Digestive_biscuit#Chocolate_digestives[chocolate biscuit].


.Docker Recap
*******************************************************************************

Docker lets us reproduce a server environment on our own machine::
    For developers, ops and infra work is always "fun",
    by which I mean a process full of fear, uncertainty, and surprisesâ€”and painfully slow too.
    Docker helps to minimise this pain by giving us a mini server on our own machine,
    which we can try things out with and get feedback quickly,
    as well as enable us to work in small steps.

`docker build && docker run`::
    We've learned the core tools for working with Docker.
    The Dockerfile specifies our image, `docker build` builds it,
    and `docker run` runs it.
    `build && run` together give us a "start again from scratch" cycle,
    which we use every time we make a code change in _src_,
    or a change in the Dockerfile.footnote:[
    There's a common pattern of mounting the whole _src_ folder into
    your Docker containers in local dev.
    It means you don't need to rebuild for every source code change.
    I didn't wan't to introduce that here because it also leads to
    subtle behaviours that can be hard to wrap your head around,
    like the _db.sqlite3_ file being shared with the container.
    For this book, the `build && run` cycle is fast enough,
    but by all means try out mounting _src_ in your own projects.]

Debugging network issues::
    We've seen how to use `curl` both outside and inside the container
    with `docker exec`.
    We've also seen the `-p` argument to bind ports inside and outside,
    and the idea of needing to bind to `0.0.0.0`.

Mounting files::
    We've also had a brief intro to mounting files from outside
    the container, into the inside.
    It's an insight into the difference between the "stateless"
    system image, and the stateful world outside of Docker.

*******************************************************************************
