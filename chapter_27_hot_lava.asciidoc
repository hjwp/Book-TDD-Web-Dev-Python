[[chapter_27_hot_lava]]
== Fast Tests, Slow Tests, and Hot Lava

[quote, 'https://www.youtube.com/watch?v=bsmFVb8guMU[Casey Kinsey]']
______________________________________________________________
The database is Hot Lava!
______________________________________________________________


((("integration tests", "vs. unit tests", secondary-sortas="unit tests")))
((("unit tests", "vs. integration tests", secondary-sortas="integration tests")))
Unless you've been through <<appendix_purist_unit_tests>>,
almost all of the "unit" tests in the book
should arguably have been called _integration_ tests,
because they rely on the database.
A "real" unit test should only engage with the unit of code under test,
and not with any external systems like databases.

If you start reading around in the world of testing and TDD,
you'll soon come across people talking about the need to have more unit tests,
and fewer integration tests and functional tests.

This is often expressed as the "Test Pyramid",
whereby your test coverage is achieved with a large base of unit tests,
a smaller number of integration tests in the middle,
and a tiny number of functional tests at the top: <<test_pyramid>>.

[[test_pyramid]]
.The Test Pyramid
image::images/test_pyramid.png["A Pyramid shape, with a large bottom layer of unit tests, a medium layer of integration tests, and a small peak of FTs"]

Let's talk about why people say this,
and the trade-offs between different types of tests.


////
PLAN:
- unit tests vs integration tests
- the problem with django
- slow tests + hot lava
- the test pyramid
- do an analysis of our app
- talk about the django sweet spot
- what do we want from tests
- pros and cons table
- advertise my book
////

.Terminology: Different Types of Test
******************************************************************************

There's no standards body that we can go to for official definitions,
so there is a certain amount of fluidity to the way people use these terms,
but here's my best attempt at reflecting a consensus view
on what the different types of tests are

Unit tests::
    The primary purpose of a unit test should be to verify the correctness
    of a single "unit" of your code
    People disagree over the optimal size of the unit,
    but they tend to agree that "the database" is not usually part of "your code".
    These tests should be "isolated" from any external 


Integration tests::
    An integration test checks that the code you control is integrated
    correctly with some external system which you don't control.


Functional tests (AKA End-to-end or System tests)::
    If an integration test checks the interaaction with _one_ external system,
    an end-to-end (aka system test) attempts to test that _all_
    the different components of the system are wired together.
    In our app, the FTs check that our HTML renders correctly in a browser,
    that any Javascript works, that our application is configured correctly,
    and that it works all the way down to the database.
    ((("system tests")))
    ((("end-to-end tests")))


There are also acceptance tests, but they're an orthogonal categorisation system.footnote:[
An acceptance test is meant to test that our system works
from the point of view of the user ("would the user accept this behaviour?").
In our example, the FTs are playing the role of acceptance tests,
because we've chosen to cover almost all of the behaviour of our application with FTs.
But, as we'll see, that's not a choice you _have_ to make,
and under a different approach approach to test coverage,
_any_ type of test can play the role of an acceptance test.
The acceptance test is the test you can point to when a stakeholder
like a product owner comes along and says
"where is this business requirement or acceptance criterion tested?".
Some tests are only really meaningful to programmers,
they are instrumental to the ultimate goal.
Others are written in a way that's much more reflective of the user experience.
]


******************************************************************************


=== What Do We Want from Our Tests, Anyway?

((("testing best practices")))
((("Test-Driven Development (TDD)", "test goals")))
Let's step back and have a think about what benefits we want our tests to deliver.
Why are we writing them in the first place?


==== Correctness

We want our application to be free of bugs--both low-level logic errors,
like off-by-one errors, and high-level bugs
like the software not ultimately delivering what our users want.
We want to find out if we ever introduce regressions
which break something that used to work,
and we want to find that out before our users see something broken.
We expect our tests to tell us our application's behaviour is correct.


==== Clean, Maintainable Code

We want our code to obey rules like YAGNI and DRY.
We want code that clearly expresses its intentions,
which is broken up into sensible components
that have well-defined responsibilities and are easily understood.
We want to keep complexity at bay, and cognitive load to a minimum.
We expect our tests to give us the confidence to refactor our application constantly,
so that we're never scared to try to improve its design,
and we would also like it if they would actively help us to find the right design.


==== Productive Workflow

Finally, we want our tests to help enable a fast and productive workflow.
We want them to help take some of the stress out of development,
and we want them to protect us from stupid mistakes.
We want them to help keep us in the "flow" state not just because we enjoy it,
but because it's highly productive.
We want our tests to give us feedback about our work as quickly as possible,
so that we can try out new ideas and evolve them quickly.
And we don't want to feel like our tests are more of a hindrance than a help
when it comes to evolving our codebase.


[[test-types-tradeoffs]]
[options="header"]
.How do different types of test help us achieve our objectives?
|================
|Objective|Some considerations

|_Correctness_
a|
* Do I have enough functional tests to reassure myself that my application _really_ works,
  from the point of view of the user?
* Am I testing all the edge cases thoroughly?
  This feels like a job for low-level, isolated tests.
* Do I have tests that check whether all my components fit together properly?
  Could some integrations tests do this, or are functional tests enough?

|_Clean, maintainable code_
a|
* Are my tests giving me the confidence to refactor my code, fearlessly and frequently?
* Are my tests helping me to drive out a good design?
  If I have a lot of integration tests and few isolated tests,
  are there any parts of my application where putting in the effort
  to write more isolated tests would give me better feedback about my design?

|_Productive workflow_
a|
* Are my feedback cycles as fast as I would like them?
  When do I get warned about bugs,
  and is there any practical way to make that happen sooner?
* If I have a lot of high-level, functional tests that take a long time to run,
  and I have to wait overnight to get feedback about accidental regressions,
  is there some way I could write some faster tests that would get me feedback quicker?
* Can I run a subset of the full test suite when I need to?
* Am I spending too much time waiting for tests to run,
  and thus less time in a productive flow state?

|================



=== Architectural Solutions


((("architectural solutions")))
((("integrated tests", "architectural considerations")))
There are also some architectural solutions
that can help to get the most out of your test suite,
and particularly that help avoid some of the disadvantages of isolated tests.

Mainly these involve trying to identify the boundaries of your system--the
points at which your code interacts with external systems,
like the database or the filesystem, or the internet, or the UI--and
trying to keep them separate from the core business logic of your application.


==== Ports and Adapters/Hexagonal/Clean Architecture

Integrated tests are most useful at the 'boundaries' of a system--at
the points where our code integrates with external systems, like a
database, filesystem, or UI components.

Similarly, it's at the boundaries that the downsides of test isolation and
mocks are at their worst, because it's at the boundaries that you're most
likely to be annoyed if your tests are tightly coupled to an implementation,
or to need more reassurance that things are integrated properly.

Conversely, code at the 'core' of our application--code that's purely
concerned with our business domain and business rules, code that's
entirely under our control--has less need for integrated
tests, since we control and understand all of it.

So one way of getting what we want is to try to minimise the amount
of our code that has to deal with boundaries. Then we test our core business
logic with isolated tests and test our integration points with integrated
tests.

Steve Freeman and Nat Pryce, in their book
<<GOOSGBT, _Growing Object-Oriented Software, Guided by Tests_>>,
call this approach "Ports and Adapters" (see <<ports-and-adapters>>).

We actually started moving towards a ports and adapters architecture in <<appendix_purist_unit_tests>>,
when we found that writing isolated unit tests was encouraging us
to push ORM code out of the main application,
and hide it in helper functions from the model layer.

This pattern is also sometimes known as the "clean architecture" or "hexagonal architecture".
See <<ch26_furtherreading>> for more info.


[[ports-and-adapters]]
.Ports and Adapters (diagram by Nat Pryce)
image::images/twp2_2601.png["Illustration of ports and adapaters architecture, with isolated core and integration points"]


==== Functional Core, Imperative Shell

Gary Bernhardt pushes this further, recommending an architecture he calls
"Functional Core, Imperative Shell", whereby the "shell" of the application,
the place where interaction with boundaries happens, follows the imperative
programming paradigm, and can be tested by integrated tests, acceptance tests,
or even (gasp!) not at all, if it's kept minimal enough. But the core of the
application is actually written following the functional programming paradigm
(complete with the "no side effects" corollary), which actually allows fully
isolated, "pure" unit tests, 'entirely without mocks'.

Check out Gary's presentation titled
https://www.youtube.com/watch?v=eOYal8elnZk["Boundaries"] for more on this
approach.



=== Conclusion

I've tried to give an overview of some of the more advanced considerations
that come into the TDD process. Mastery of these topics is something
that comes from long years of practice, and I'm not there yet, by any means. So
I heartily encourage you to take everything I've said with a pinch of salt, to
go out there, try various approaches, listen to what other people have to say
too, and find out what works for you.

Here are some places to go for further reading.
((("Test-Driven Development (TDD)", "additional resources")))

[[ch26_furtherreading]]
==== Further Reading

Fast Test, Slow Test and Boundaries::
    Gary Bernhardt's talks from Pycon
    https://www.youtube.com/watch?v=RAxiiRPHS9k[2012] and
    https://www.youtube.com/watch?v=eOYal8elnZk[2013].  His
    http://www.destroyallsoftware.com[screencasts] are also well worth a look.

Ports and Adapters::
    Steve Freeman and Nat Pryce wrote about this in <<GOOSGBT, their book>>.
    You can also catch a good discussion in
    http://vimeo.com/83960706[this talk]. See also
    http://blog.8thlight.com/uncle-bob/2012/08/13/the-clean-architecture.html[Uncle
    Bob's description of the clean architecture], and
    http://alistair.cockburn.us/Hexagonal+architecture[Alistair Cockburn
    coining the term "hexagonal architecture"].

Hot Lava::
    https://www.youtube.com/watch?v=bsmFVb8guMU[Casey Kinsey's memorable
    phrase] encouraging you to avoid touching the database, whenever you can.

Inverting the Pyramid::
    The idea that projects end up with too great a ratio of slow, high-level
    tests to unit tests, and a
    http://watirmelon.com/tag/testing-pyramid/[visual metaphor for the effort
    to invert that ratio].

Integrated tests are a scam::
    J.B. Rainsberger has a
    http://blog.thecodewhisperer.com/2010/10/16/integrated-tests-are-a-scam/[famous rant]
    about the way integrated tests will ruin your life.
    Then check out a couple of follow-up posts, particularly
    http://www.jbrains.ca/permalink/using-integration-tests-mindfully-a-case-study[this
    defence of acceptance tests] (what I call functional tests), and
    http://www.jbrains.ca/permalink/part-2-some-hidden-costs-of-integration-tests[this
    analysis of how slow tests kill productivity].
    ((("integrated tests", "benefits and drawbacks of")))

The Test-Double testing wiki::
    Justin Searls's online resource is a great source of definitions
    and discussions of testing pros and cons,
    and arrives at its own conclusions of the right way to do things:
    https://github.com/testdouble/contributing-tests/wiki/Test-Driven-Development[testing wiki].

A pragmatic view::
    Martin Fowler (author of 'Refactoring') presents a
    http://martinfowler.com/bliki/UnitTest.html[reasonably balanced, pragmatic approach].


.On Getting the Balance Right Between Different Types of Test
******************************************************************************
Start out by being pragmatic::
    Spending a long time agonising about what kinds of test to write
    is a great way to prevaricate.
    Better to start by writing whichever type of test occurs to you first,
    and change it later if you need to.
    Learn by doing.

Focus on what you want from your tests::
    Your objectives are 'correctness', 'good design', and 'fast feedback cycles'.
    Different types of test will help you achieve each of these in different measures.
    <<test-types-tradeoffs>> has some good questions to ask yourself.

Architecture matters::
    Your architecture to some extent dictates the types of tests that you need.
    The more you can separate your business logic from your external dependencies,
    and the more modular your code, the closer you'll get to a nice balance
    between unit tests, integration tests and end-to-end tests.
******************************************************************************

