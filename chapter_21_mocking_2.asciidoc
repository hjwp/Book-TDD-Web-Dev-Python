[[chapter_21_mocking_2]]
== Using Mocks for Test Isolation

In this chapter, we'll finish up our login system.
While doing so, we'll explore an alternative use of mocks:
to isolate parts of the system from each other. This
enables more targeted testing, fights combinatorial explosion,
and reduces duplication between tests.


NOTE: In this chapter, we start to drift towards what's called "London-school TDD",
    which is a variant on the "Classical" or "Detroit" style of TDD
    that I mostly show in the book.
    We won't get into the details here,
    but London-school TDD places more emphasis on mocking and isolating parts of the system.
    As always, there are pros and cons!
    Read more at 
    https://www.obeythetestinggoat.com/book/appendix_purist_unit_tests.html[Online Appendix: Test Isolation and "Listening to Your Tests"].


Along the way, we'll learn a few more useful features of `unittest.mock`,
and we'll also have a discussion about how many tests are "enough".



=== Using Our Auth Backend in the Login View

[role="scratchpad"]
*****
* _[strikethrough line-through]#Token model with email and UID#_
* _[strikethrough line-through]#View to create token and send login email incl. url w/ token UID#_
* _[strikethrough line-through]#Custom user model with USERNAME_FIELD=email#_
* _[strikethrough line-through]#Authentication backend with authenticate() and get_user() functions#_
* _Registering auth backend in settings.py_
* _Login view calls authenticate() and login() from django.contrib.auth_
* _Logout view calls django.contrib.auth.logout_
*****

We got our auth backend ready in the last chapter;
now we need use the backend in our login view.
But first, as our scratchpad says, we need to add it to _settings.py_:


[role="sourcecode"]
.src/superlists/settings.py (ch21l001)
====
[source,python]
----
AUTH_USER_MODEL = "accounts.User"
AUTHENTICATION_BACKENDS = [
    "accounts.authentication.PasswordlessAuthenticationBackend",
]

[...]
----
====

That was easy!

[role="scratchpad"]
*****
* _[strikethrough line-through]#Token model with email and UID#_
* _[strikethrough line-through]#View to create token and send login email incl. url w/ token UID#_
* _[strikethrough line-through]#Custom user model with USERNAME_FIELD=email#_
* _[strikethrough line-through]#Authentication backend with authenticate() and get_user() functions#_
* _[strikethrough line-through]#Registering auth backend in settings.py#_
* _Login view calls authenticate() and login() from django.contrib.auth_
* _Logout view calls django.contrib.auth.logout_
*****

Next, let's write some tests for what should happen in our view.
Looking back at the spike again:

[role="sourcecode skipme"]
.src/accounts/views.py
====
[source,python]
----
def login(request):
    print("login view", file=sys.stderr)
    uid = request.GET.get("uid")
    user = auth.authenticate(uid=uid)
    if user is not None:
        auth.login(request, user)
    return redirect("/")
----
====

TIP: You can view the contents of files from the spike
    using, for example, `git show passwordless-spike:src/accounts/views.py`.

We call `django.contrib.auth.authenticate` and then,
if it returns a user, we call `django.contrib.auth.login`.

TIP: This is a good time to check out the
    https://docs.djangoproject.com/en/5.2/topics/auth/default/#how-to-log-a-user-in[Django docs on authentication]
    for a little more context.
    ((("Django framework", "documentation")))


==== Straightforward Non-Mocky Test for Our View

Here's the most obvious test we might want to write,
thinking in terms of the _behaviour_ we want:

* If someone has a valid token, they should get logged in.
* If someone tries to use an invalid token (or does not have one), it should not log them in.


Here's how we might add the happy-path test for the user with the valid token:

[role="sourcecode"]
.src/accounts/tests/test_views.py (ch21l002)
====
[source,python]
----
from django.contrib import auth
[...]

class LoginViewTest(TestCase):
    def test_redirects_to_home_page(self):
        [...]

    def test_logs_in_if_given_valid_token(self):
        anon_user = auth.get_user(self.client)  # <1>
        self.assertEqual(anon_user.is_authenticated, False)  # <2>

        token = Token.objects.create(email="edith@example.com")
        self.client.get(f"/accounts/login?token={token.uid}")

        user = auth.get_user(self.client)
        self.assertEqual(user.is_authenticated, True)  # <3>
        self.assertEqual(user.email, "edith@example.com")  # <3>
----
====

<1> We use Django's `auth.get_user()` to extract the current user from the test client.

<2> We verify we're not logged in before we start. (This isn't strictly necessary, but it's always nice to know you're on firm ground.)

<3> And here's where we check that we've been logged in,
    with a user with the right email address.

And that will fail as expected:

----
    self.assertEqual(user.is_authenticated, True)
AssertionError: False != True
----

[role="pagebreak-before"]
We can get it to pass by "cheating", like this:


[role="sourcecode small-code"]
.src/accounts/views.py (ch21l003)
====
[source,python]
----
from django.contrib import auth, messages
[...]
from accounts.models import Token, User


def send_login_email(request):
    [...]


def login(request):
    user = User.objects.create(email="edith@example.com")
    auth.login(request, user)
    return redirect("/")
----
====

...

----
OK
----

That forces us to write another test:



[role="sourcecode"]
.src/accounts/tests/test_views.py (ch21l004)
====
[source,python]
----
def test_shows_login_error_if_token_invalid(self):
    response = self.client.get("/accounts/login?token=invalid-token", follow=True)
    user = auth.get_user(self.client)
    self.assertEqual(user.is_authenticated, False)
    message = list(response.context["messages"])[0]
    self.assertEqual(
        message.message,
        "Invalid login link, please request a new one",
    )
    self.assertEqual(message.tags, "error")
----
====

And now we get that passing by using the most straightforward implementation...


[role="sourcecode small-code"]
.src/accounts/views.py (ch21l005)
====
[source,python]
----
def login(request):
    if Token.objects.filter(uid=request.GET["token"]).exists():  # <1>
        user = User.objects.create(email="edith@example.com")  # <2> <3>
        auth.login(request, user)
    else:
        messages.error(request, "Invalid login link, please request a new one")  # <4>
    return redirect("/")
----
====

<1> Oh wait; we forgot about our authentication backend
    and just did the query directly from the token model!
    Well that's arguably more straightforward,
    but how do we force ourselves to write the code the way we want to—i.e.,
    using Django's authentication API?

<2> Oh dear, and the email address is still hardcoded.
    We might have to think about writing an extra test to force ourselves to fix that.


<3> Oh--also, we're hardcoding the creation of a user every time,
    but actually, we want to have the get-or-create logic
    that we implemented in our backend.

<4> This bit is OK at least!

Is this starting to feel a bit familiar?
We've already written all the tests for the various permutations of our authentication logic,
and we're considering writing equivalent tests at the views layer.


=== Combinatorial Explosion

<<table-21-1>> recaps the tests we might want to write at each layer in our application.((("combinatorial explosion")))

[[table-21-1]]
.What we want to test in each layer
|=======
|Views layer| Authentication backend | Models layer

a| * Valid token means user is logged in
  * Invalid token means user is not logged in

a| * Returns correct existing user for a valid token
  * Creates a new user for a new email address
  * Returns `none` for an invalid token

a| * Token associates email and UID
  * User can be retrieved from token UID
|=======

We already have three tests in the models layer, and five in the authentication layer.
We started off writing the tests in the views layer,
where—_conceptually_—we only really want two test cases,
and we're finding ourselves wondering if we need to write
a whole bunch of tests that essentially duplicate the authentication layer tests. This is an example of the _combinatorial explosion_ problem.


==== The Car Factory Example

Imagine we're testing a car factory:

* First, we choose the car type: normal, station-wagon, or convertible.
* Then, we choose the engine type: petrol, diesel, or electric.
* Finally, we choose the colour: red, white, or hot pink.

[role="pagebreak-before"]
Here's how it might look in code:

[role="skipme"]
[source,python]
----
def build_car(car_type, engine_type, colour):
    engine = _create_engine(engine_type)
    naked_car = _assemble_car(engine, car_type)
    finished_car = _paint_car(naked_car, colour)
    return finished_car
----

How many tests do we need?  Well, the upper bound to test every possible combination
is 3 &times; 3 &times; 3 = 27 tests.  That's a lot!

How many tests do we _actually_ need to write?
Well, it depends on how we're testing, how the different parts of the factory are integrated,
and what we know about the system. Do we need to test every single colour? Maybe!
Or, maybe, if we're happy that we can do two different colours, then we're happy that we can do any number—whether it's two, three, or hundreds.  Perhaps we need two tests, maybe three.

OK, but do we need to test that painting works for all the different engine types?
Well, the painting process is probably independent of engine type:
if we can paint a diesel in red, we can paint it in pink or white too.

But, perhaps it _is_ affected by the car type:
painting a convertible with a fabric roof
might be a very different technological process to painting a hard-bodied car. So, we'd probably want to test that painting _in general_ works for each car type (three tests),
but we don't need to test that painting works for every engine type.

What we're analysing here is the level of "coupling" between the different parts of the system.
Painting is tightly coupled to car type, but not to engine type.
Painting "needs to know" about car types, but it does not "need to know" about engine types.


TIP: The more tightly coupled two parts of the system are,
    the more tests you'll need to write to cover all the combinations of their behaviour.

Another way of thinking about it is: what level are we writing tests at?
You can choose to write low-level tests that cover only one part of the assembly process,
or higher-level ones that test several steps together—or perhaps all of them end-to-end.
See <<car-factory-illustration>>.

[[car-factory-illustration]]
.Analysing how many tests are needed at different levels
image::images/tdd3_2101.png["An illustration of the car factory, with boxes for each step in the process (build engine, assemble, paint), and descriptions of testing each step separately vs testing them in combination."]
// CSANAD: just a tiny thing: in the diagram, below the "Paint" box, there is
// an apostrophe missing in "engine type doesn't matter".

// SEBASTIAN: How about splitting this big image into several smaller ones? At the first encounter, I skipped it only to discover I need to jump up and down to have visualizations of paragraphs below.
//      Not a showstopper, tho.

Analysing things in these terms,
we think about the inputs and outputs that apply to each type of test,
as well as which attributes of the inputs matter, and which don't.

Testing the first stage of the process—building the engine—is straightforward.  The "engine type" input has three possible values, so we need three tests of the output, which is the engine.
If we're testing at the end-to-end level, no matter how many tests we have in total,
we know we'll need at least three to be the tests
that check if we can produce a car with a working engine of each type.

Testing the painting needs a bit more thought.
If we test at the low level, the inputs are a naked car and a paint colour.
There are theoretically nine types of naked car; do we need to test all of them?
No. The engine type doesn't matter; we only need to test one of each body type.
Does that mean 3 &times; 3 = 9 tests?  No.  The colour and body type are independent.
We can just test that all three colours work, and that all three body types work—so that's six tests.

What about at the end-to-end level?
It depends if we're being rigorous about "closed-box" testing,
where we're not supposed to know anything about how the production process works.
In that case, maybe we _do_ need 27 tests.
But if we allow that we know about the internals,
then we can apply similar reasoning to what we used at the lower level.
However many tests we end up with,
we need three of them to be checking each colour,
and three that check that each body type can be painted.

Let's see if we can apply this sort of analysis to our authentication system.


=== Using Mocks to Test Parts of Our System in Isolation

To recap, so far we have some minimal tests at the models layer,
and we have comprehensive tests of our authentication backend,
and we're now wondering how many tests we need at the views layer.


Here's the current state of our view:

[role="sourcecode currentcontents"]
.src/accounts/views.py
====
[source,python]
----
def login(request):
    if Token.objects.filter(uid=request.GET["token"]).exists():
        user = User.objects.create(email="edith@example.com")
        auth.login(request, user)
    else:
        messages.error(request, "Invalid login link, please request a new one")
    return redirect("/")
----
====

We know we want to transform it to something like this:


[role="sourcecode skipme small-code"]
.src/accounts/views.py
====
[source,python]
----
def login(request):
    if user := auth.authenticate(uid=request.GET.get("token"))  # <1>
        auth.login(request, user)  # <2>
    else:
        messages.error(request, "Invalid login link, please request a new one")  # <3>

    return redirect("/")
----
====

<1> We want to refactor our logic to use the `authenticate()` function
    from our backend.  Really good place for a walrus (`:=`) too!
<2> We have the happy path where the user gets logged in.
<3> We have the unhappy path where the user gets an error message instead.

But currently, our tests are letting us "get away" with
the wrong implementation. Here are three possible options for getting ourselves to the right state:

1. Add more tests for all possible combinations at the view level
  (token exists but no user, token exists for an existing user, invalid token,
  etc.), until we end up duplicating all the logic in the auth backend in our view—and then feel justified in refactoring across to just calling the auth backend.

2. Stick with our current two tests, and decide it's OK to refactor already.

3. Test the view in isolation, using mocks to verify that we call the auth backend.


Each option has pros and cons!  If I was going for option (1),
essentially going all in on test coverage at the views layer,
I'd probably think about deleting all the tests at the auth layer afterwards.

If you were to ask me what my personal preference or instinctive choice would be,
I'd say at this point it might be to go with (2),
and say with one happy-path and one unhappy-path test,
we're OK to refactor and switch across already.

But because this chapter is about mocks, let's investigate option (3) instead.
Besides, it'll be an excuse to do fun things with them,
like playing with `.return_value`.

((("mocks", "reducing duplication with", id="Mreduce19")))
((("duplication, eliminating", id="dupel19")))
So far, we've used mocks to test external dependencies,
like Django's mail-sending function.
The main reason to use a mock we've discussed so far is to isolate ourselves from external side effects—in this case, to avoid sending out actual emails during our tests.

In this section, we'll look at a different possible use case for mocks: testing parts of our _own_ code in isolation from each other,
as a way of reducing duplication and avoiding combinatorial explosion in our tests.


==== Mocks Can Also Let You Test the Implementation, When It Matters


On top of that, the fact that we're using the Django `auth.authenticate` function
rather than calling our own code directly is relevant.
Django has already introduced an abstraction:
to decouple the specifics of authentication backends
from the views that use them.
This makes it easier for us to add further backends in future.

So in this case
(in contrast to the example in  <<mocks-tightly-coupled-sidebar>>)
the implementation _does_ matter,
because we've decided to use a particular, specific interface to implement our authentication system. This is something we might want to document and verify in our tests—and mocks are one way to enable that.

// SEBASTIAN: I am missing one crucial sentence here - that this Django-provided abstraction IS STABLE, so it's safe to mock it.
//      This is part of a public Django API, meaning it's not going anywhere soon or without breaking backwards-compatibility. That would of course be not welcomed by Django users :)
// HARRY - otoh, "don't mock what you don't own".
// some ppl would say, better to write your own wrapper around any third party api,
// and then your mock doesn't need to change if the third party api changes.

[role="pagebreak-before less_space"]
=== Starting Again: Test-Driving Our Implementation with Mocks

Let's see how things would look if we had decided to test-drive our implementation with mocks in the first place.
We'll start by reverting all the authentication stuff,
both from our test and from our view.

Let's disable the test first (we can re-enable them later to sense-check things):

[role="sourcecode small-code"]
.src/accounts/tests/test_views.py (ch21l006)
====
[source,python]
----
class LoginViewTest(TestCase):
    def test_redirects_to_home_page(self):  <1>
        [...]
    def DONT_test_logs_in_if_given_valid_token(self):  <2>
        [...]
    def DONT_test_shows_login_error_if_token_invalid(self):  <2>
        [...]
----
====

<1> We can leave the test for the redirect, as that doesn't involve the auth framework.
<2> We change the test name so it no longer starts with `test_`,
    using a highly noticeable set of capital letters
    so we don't forget to come back and re-enable them later.
    I call this "DONTifying" tests. :)


Now let's revert the view, and replace our hacky code with some to-dos:

[role="sourcecode"]
.src/accounts/views.py (ch21l007)
====
[source,python]
----
# from django.contrib import auth, messages  # <1>
from django.contrib import messages
[...]


def login(request):
    # TODO: call authenticate(),  # <2>
    # then auth.login() with the user if we get one,
    # or messages.error() if we get None.
    return redirect("/")
----
====

<1> In order to demonstrate a common error message shortly,
    I'm also reverting our import of the `contrib.auth` module.

<2> And here's where we delete our first implementation
    and replace it with some to-dos.

[role="pagebreak-before"]
Let's check that all our tests pass:


[subs="specialcharacters,macros"]
----
$ pass:quotes[*python src/manage.py test accounts*]
[...]
Ran 15 tests in 0.021s

OK
----


Now let's start again with mock-based tests.
First, we can write a test that makes sure we call `authenticate()` correctly:

[role="sourcecode small-code"]
.src/accounts/tests/test_views.py (ch21l008)
====
[source,python]
----
class LoginViewTest(TestCase):
    [...]

    @mock.patch("accounts.views.auth")  # <1>
    def test_calls_authenticate_with_uid_from_get_request(self, mock_auth):  # <2>
        self.client.get("/accounts/login?token=abcd123")
        self.assertEqual(
            mock_auth.authenticate.call_args,  # <3>
            mock.call(uid="abcd123"),  # <4>
        )
----
====

<1> We expect to be using the `django.contrib.auth` module in _views.py_,
    and we mock it out here.  Note that this time, we're not mocking out
    a function; we're mocking out a whole module, and thus implicitly
    mocking out all the functions (and any other objects) that module contains.

<2> As usual, the mocked object is injected into our test method.

<3> This time, we've mocked out a module rather than a function.
    So we examine the `call_args`—not of the `mock_auth` module,
    but of the `mock_auth.authenticate` function.
    Because all the attributes of a mock are more mocks, that's a mock too.
    You can start to see why `Mock` objects are so convenient,
    compared to trying to build your own.

<4> Now, instead of "unpacking" the call args, we use the `call` function
    for a neater way of saying what it should have been called with--that is,
    the token from the GET request.
    (See <<mock-call-args-sidebar>>.)


[role="less_space pagebreak-before"]
[[mock-call-args-sidebar]]
.On Mock call_args
*******************************************************************************

((("call_args property")))
The `.call_args` property on a mock represents the positional and keyword arguments
that the mock was called with.
It's a special "call" object type,
which is essentially a tuple of `(positional_args, keyword_args)`.
`positional_args` is itself a tuple,
consisting of the set of positional arguments.
`keyword_args` is a dictionary. Here they all are in action:

[role="small-code skipme"]
[source,python]
----
>>> from unittest.mock import Mock, call
>>> m = Mock()
>>> m(42, 43, 'positional arg 3', key='val', thing=666)
<Mock name='mock()' id='139909729163528'>

>>> m.call_args
call(42, 43, 'positional arg 3', key='val', thing=666)

>>> m.call_args == ((42, 43, 'positional arg 3'), {'key': 'val', 'thing': 666})
True
>>> m.call_args == call(42, 43, 'positional arg 3', key='val', thing=666)
True
----

So in our test,  we could have done this instead:

[role="sourcecode skipme"]
.src/accounts/tests/test_views.py
====
[source,python]
----
    self.assertEqual(
        mock_auth.authenticate.call_args,
        ((,), {'uid': 'abcd123'})
    )
    # or this
    args, kwargs = mock_auth.authenticate.call_args
    self.assertEqual(args, (,))
    self.assertEqual(kwargs, {'uid': 'abcd123'})
----
====

But you can see how using the `call` helper is nicer.

See also <<avoid-assert-called-with-sidebar>>,
for some discussion of `call_args` versus the magic `assert_called_with` methods.

*******************************************************************************


What happens when we run the test?   The first error is this:

[subs="specialcharacters,macros"]
----
$ pass:quotes[*python src/manage.py test accounts*]
[...]
AttributeError: <module 'accounts.views' from
'...goat-book/src/accounts/views.py'> does not have the attribute 'auth'
----

TIP: `module foo does not have the attribute bar`
    is a common first failure in a test that uses mocks.
    It's telling you that you're trying to mock out something
    that doesn't yet exist (or isn't yet imported)
    in the target module.


Once we reimport `django.contrib.auth`, the error changes:


[role="sourcecode"]
.src/accounts/views.py (ch21l009)
====
[source,python]
----
from django.contrib import auth, messages
[...]
----
====

Now we get:


[subs="specialcharacters,macros"]
----
FAIL: test_calls_authenticate_with_uid_from_get_request [...]
[...]
AssertionError: None != call(uid='abcd123')
----

It's telling us that the view doesn't call the `auth.authenticate` function at all.
Let's fix that, but get it deliberately wrong, just to see:


[role="sourcecode"]
.src/accounts/views.py (ch21l010)
====
[source,python]
----
def login(request):
    # TODO: call authenticate(),
    auth.authenticate("bang!")
    # then auth.login() with the user if we get one,
    # or messages.error() if we get None.
    return redirect("/")
----
====


Bang, indeed!

[subs="specialcharacters,macros"]
----
$ pass:quotes[*python src/manage.py test accounts*]
[...]
AssertionError: call('bang!') != call(uid='abcd123')
[...]
FAILED (failures=1)
----

Let's give `authenticate` the arguments it expects then:


[role="sourcecode"]
.src/accounts/views.py (ch21l011)
====
[source,python]
----
def login(request):
    # TODO: call authenticate(),
    auth.authenticate(uid=request.GET["token"])
    # then auth.login() with the user if we get one,
    # or messages.error() if we get None.
    return redirect("/")
----
====

[role="pagebreak-before"]
That gets us to passing tests:


[subs="specialcharacters,macros"]
----
$ pass:quotes[*python src/manage.py test accounts*]
Ran 16 tests in 0.023s

OK
----

==== Using mock.return_value

((("mocks", "mock.return_value")))
Next, we want to check that if the authenticate function returns a user,
we pass that into `auth.login`.  Let's see how that test looks:


[role="sourcecode"]
.src/accounts/tests/test_views.py (ch21l012)
====
[source,python]
----
@mock.patch("accounts.views.auth")  # <1>
def test_calls_auth_login_with_user_if_there_is_one(self, mock_auth):
    response = self.client.get("/accounts/login?token=abcd123")
    self.assertEqual(
        mock_auth.login.call_args,  # <2>
        mock.call(
            response.wsgi_request,  # <3>
            mock_auth.authenticate.return_value,  # <4>
        ),
    )
----
====

<1> We mock the `contrib.auth` module again.

<2> This time we examine the call args for the `auth.login` function.

<3> We check that it's called with the request object that the view sees...

<4> ...and we check that the second argument was
    "whatever the `authenticate()` function returned".
    Because `authenticate()` is also mocked out,
    we can use its special `.return_value` attribute.
    We know that, in real life, that will be a user object.
    But in this test, it's all mocks.
    Can you see what I mean about mocky tests being hard to understand sometimes?

When you call a mock, you get another mock.
But you can also get a copy of that returned mock from the original mock that you called.
Boy, it sure is hard to explain this stuff without saying "mock" a lot!
Another little console illustration might help:

[role="skipme"]
[source,python]
----
>>> m = Mock()
>>> thing = m()
>>> thing
<Mock name='mock()' id='140652722034952'>
>>> m.return_value
<Mock name='mock()' id='140652722034952'>
>>> thing == m.return_value
True
----


[[avoid-assert-called-with-sidebar]]
.Avoid Mock's Magic assert_called...Methods?
*******************************************************************************

If you've used `unittest.mock` before, you may have come across its special
`assert_called...`
https://docs.python.org/3/library/unittest.mock.html#unittest.mock.Mock.assert_called[methods],
and you may be wondering why I didn't use them.

For example, instead of doing:

[role="skipme"]
[source,python]
----
self.assertEqual(a_mock.call_args, call(foo, bar))
----

You can just do:

[role="skipme"]
[source,python]
----
a_mock.assert_called_with(foo, bar)
----

And the _mock_ library will raise an `AssertionError` for you if there is a
mismatch.

Why not use that?  For me, the problem with these magic methods is that
it's too easy to make a silly typo and end up with a test that always passes:

[role="skipme"]
[source,python]
----
a_mock.asssert_called_with(foo, bar)  # will always pass
----

Unless you get the magic method name exactly right,footnote:[
There was actually an attempt to mitigate this problem in Python 3.5,
with the addition of an `unsafe` argument that defaults to `False`,
which will cause the mock to raise `AttributeError` for some common
misspellings of `assert_`.  Just not, for example, the one I'm using here—so I prefer not to rely on that. More info in the https://docs.python.org/3/library/unittest.mock.html#unittest.mock.Mock[Python docs].]
it will just silently return another mock,
and you may not realise that you've written a test that tests nothing at all. That's why I prefer to always have an explicit `unittest` method in there.footnote:[
If you're using Pytest, there's an additional benefit to seeing the `assert` keyword
rather than a normal method call: it makes the assert pop out.]

*******************************************************************************


In any case, what do we get from running the test?

[subs="specialcharacters,macros"]
----
$ pass:quotes[*python src/manage.py test accounts*]
[...]
AssertionError: None != call(<WSGIRequest: GET '/accounts/login?t[...]
----

Sure enough, it's telling us that we're not calling `auth.login()` at all yet.
Let's first try doing that deliberately wrong as usual!


[role="sourcecode"]
.src/accounts/views.py (ch21l013)
====
[source,python]
----
def login(request):
    # TODO: call authenticate(),
    auth.authenticate(uid=request.GET["token"])
    # then auth.login() with the user if we get one,
    auth.login("ack!")
    # or messages.error() if we get None.
    return redirect("/")
----
====

Ack, indeed!

[subs="specialcharacters,macros"]
----
$ pass:quotes[*python src/manage.py test accounts*]
[...]

ERROR: test_redirects_to_home_page
[...]
TypeError: login() missing 1 required positional argument: 'user'

FAIL: test_calls_auth_login_with_user_if_there_is_one [...]
[...]
AssertionError: call('ack!') != call(<WSGIRequest: GET
'/accounts/login?token=[...]
[...]

Ran 17 tests in 0.026s

FAILED (failures=1, errors=1)
----

That's one expected failure from our mocky test,
and one (more) unexpected failure from the non-mocky test.

Let's see if we can fix them:

[role="sourcecode"]
.src/accounts/views.py (ch21l014)
====
[source,python]
----
def login(request):
    # TODO: call authenticate(),
    user = auth.authenticate(uid=request.GET["token"])
    # then auth.login() with the user if we get one,
    auth.login(request, user)
    # or messages.error() if we get None.
    return redirect("/")
----
====


Well, that does fix our mocky test, but not the other one;
it now has a slightly different complaint:

[subs="specialcharacters,macros"]
----
ERROR: test_redirects_to_home_page
(accounts.tests.test_views.LoginViewTest.test_redirects_to_home_page)
[...]
  File "...goat-book/src/accounts/views.py", line 33, in login
    auth.login(request, user)
[...]
AttributeError: 'AnonymousUser' object has no attribute '_meta'
----

It's because we're still calling `auth.login` indiscriminately on any kind of user,
and that's causing problems back in our original test for the redirect,
which _isn't_ currently mocking out `auth.login`.

[role="pagebreak-before"]
We can get back to passing like this:


[role="sourcecode"]
.src/accounts/views.py (ch21l015)
====
[source,python]
----
def login(request):
    # TODO: call authenticate(),
    if user := auth.authenticate(uid=request.GET["token"]):
        # then auth.login() with the user if we get one,
        auth.login(request, user)
----
====


This gets our unit test passing:

[subs="specialcharacters,quotes"]
----
$ *python src/manage.py test accounts*
[...]

OK
----


==== Using .return_value During Test Setup

I'm a little nervous that we've introduced an `if` without an _explicit_ test for it.
Testing the unhappy path will reassure me.
We can use our existing test for the error case to crib from.

We want to be able to set up our mocks to say:
`auth.authenticate()` should return `None`.
We can do that by setting the `.return_value` on the mock:


[role="sourcecode"]
.src/accounts/tests/test_views.py (ch21l016)
====
[source,python]
----
    @mock.patch("accounts.views.auth")
    def test_adds_error_message_if_auth_user_is_None(self, mock_auth):
        mock_auth.authenticate.return_value = None  # <1>

        response = self.client.get("/accounts/login?token=abcd123", follow=True)

        message = list(response.context["messages"])[0]
        self.assertEqual(  # <2>
            message.message,
            "Invalid login link, please request a new one",
        )
        self.assertEqual(message.tags, "error")
----
====

<1> We use `.return_value` on our mock once again.
    But this time, we assign to it _before_ it's used
    (in the setup part of the test—aka the "arrange" or "given" phase),
    rather than reading from it (in the assert/“when” part),
    as we did earlier.

<2> Our asserts are copied across from the existing test for the error case,
    `DONT_test_shows_login_error_if_token_invalid()`.

[role="pagebreak-before"]
That gives us this somewhat cryptic, but expected failure:

----
ERROR: test_adds_error_message_if_auth_user_is_None [...]
[...]
    message = list(response.context["messages"])[0]
              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^
IndexError: list index out of range
----

Essentially, that's saying there are no messages in our response. We can get it passing like this, starting with a deliberate mistake as always:

[role="sourcecode"]
.src/accounts/views.py (ch21l017)
====
[source,python]
----
def login(request):
    # TODO: call authenticate(),
    if user := auth.authenticate(uid=request.GET["token"]):
        # then auth.login() with the user if we get one,
        auth.login(request, user)
    else:
        # or messages.error() if we get None.
        messages.error(request, "boo")
    return redirect("/")
----
====

Which gives us:

----
AssertionError: 'boo' != 'Invalid login link, please request a new one'
----

And so:


[role="sourcecode"]
.src/accounts/views.py (ch21l018)
====
[source,python]
----
def login(request):
    # TODO: call authenticate(),
    if user := auth.authenticate(uid=request.GET["token"]):
        # then auth.login() with the user if we get one,
        auth.login(request, user)
    else:
        # or messages.error() if we get None.
        messages.error(request, "Invalid login link, please request a new one")
    return redirect("/")
----
====

Now our tests pass:

[subs="specialcharacters,quotes"]
----
$ *python src/manage.py test accounts*
[...]

Ran 18 tests in 0.025s

OK
----

[role="pagebreak-before"]
And we can do a final refactor to remove those comments:



[role="sourcecode"]
.src/accounts/views.py (ch21l019)
====
[source,python]
----
from accounts.models import Token  # <1>
[...]


def login(request):  # <2>
    if user := auth.authenticate(uid=request.GET["token"]):
        auth.login(request, user)
    else:
        messages.error(request, "Invalid login link, please request a new one")
    return redirect("/")
----
====

<1> We no longer need to explicitly import the user model
<2> and our view is down to just five lines.

Lovely!  What's next?
((("", startref="Mreduce19")))((("", startref="dupel19")))


==== UnDONTifying

Remember we still have the DONTified, non-mocky tests?
Let's re-enable now to sense-check that our mocky tests have driven
us to the right place:


[role="sourcecode small-code"]
.src/accounts/tests/test_views.py (ch21l020)
====
[source,diff]
----
@@ -63,7 +63,7 @@ class LoginViewTest(TestCase):
         response = self.client.get("/accounts/login?token=abcd123")
         self.assertRedirects(response, "/")

-    def DONT_test_logs_in_if_given_valid_token(self):
+    def test_logs_in_if_given_valid_token(self):
         anon_user = auth.get_user(self.client)
         self.assertEqual(anon_user.is_authenticated, False)

@@ -74,7 +74,7 @@ class LoginViewTest(TestCase):
         self.assertEqual(user.is_authenticated, True)
         self.assertEqual(user.email, "edith@example.com")

-    def DONT_test_shows_login_error_if_token_invalid(self):
+    def test_shows_login_error_if_token_invalid(self):
         response = self.client.get("/accounts/login?token=invalid-token", follow=True)
----
====


Sure enough, they both pass:


[subs="specialcharacters,quotes"]
----
$ *python src/manage.py test accounts*
[...]
Ran 20 tests in 0.025s

OK
----


=== Deciding Which Tests to Keep


We now definitely have duplicate tests:


[role="sourcecode skipme"]
.src/accounts/tests/test_views.py
====
[source,python]
----
class LoginViewTest(TestCase):
    def test_redirects_to_home_page(self):
        [...]

    def test_logs_in_if_given_valid_token(self):
        [...]

    def test_shows_login_error_if_token_invalid(self):
        [...]

    @mock.patch("accounts.views.auth")
    def test_calls_authenticate_with_uid_from_get_request(self, mock_auth):
        [...]

    @mock.patch("accounts.views.auth")
    def test_calls_auth_login_with_user_if_there_is_one(self, mock_auth):
        [...]

    @mock.patch("accounts.views.auth")
    def test_adds_error_message_if_auth_user_is_None(self, mock_auth):
        [...]
----
====

The redirect test could stay the same whether we're using mocks or not.
We then have two non-mocky tests for the happy and unhappy paths,
and three mocky tests:

. One checks that we are integrated with our auth backend correctly.
. One checks that we call the built-in `auth.login` function correctly,
  which tests the happy path.
. And one checks we set an error message in the unhappy path.

I think there are lots of ways to justify different choices here,
but my instinct tends to be to avoid using mocks if you can.
So, I propose we delete the two mocky tests for the happy and unhappy paths,
as they are reasonably covered by the non-mocky ones.
But I think we can justify keeping the first mocky test,
because it adds value by checking that we're doing our authentication
the "right" way—i.e., by calling into Django's `auth.authenticate()` function
(instead of, for example, instantiating and calling our auth backend ourselves,
or even just implementing authentication inline in the view).


TIP: "Test behaviour, not implementation" is a GREAT rule of thumb for tests.
    But sometimes, the fact that you're using one implementation rather than another
    really is important.  In these cases, a mocky test can be useful.


So let's delete our last two mocky tests.
I'm also going to rename the remaining one to make our intention clear;
we want to check we are using the Django auth library:


[role="sourcecode"]
.src/accounts/tests/test_views.py (ch21l021)
====
[source,python]
----
    @mock.patch("accounts.views.auth")
    def test_calls_django_auth_authenticate(self, mock_auth):
        [...]
----
====
// CSANAD: I think the `diff` style snippets are better for renaming things.

And we're down to 17 tests:

[subs="specialcharacters,quotes"]
----
$ *python src/manage.py test accounts*
[...]
Ran 18 tests in 0.015s

OK
----


=== The Moment of Truth:  Will the FT Pass?

((("mocks", "functional test for")))
((("functional tests (FTs)", "for mocks", secondary-sortas="mocks")))
We're just about ready to try our functional test!
Let's just make sure our base template shows a different navbar
for logged-in and non–logged-in users.
Our FT relies on being able to see the user's email in the navbar
in the logged-in state, and it needs a "Log out" button too:

[role="sourcecode small-code"]
.src/lists/templates/base.html (ch21l022)
====
[source,html]
----
<nav class="navbar">
  <div class="container-fluid">
    <a class="navbar-brand" href="/">Superlists</a>
    {% if user.email %}  <1>
      <span class="navbar-text">Logged in as {{ user.email }}</span>
      <form method="POST" action="TODO">
        {% csrf_token %}
        <button id="id_logout" class="btn btn-outline-secondary" type="submit">
          Log out
        </button>
      </form>
    {% else %}
      <form method="POST" action="{% url 'send_login_email' %}">
        <div class="input-group">
          <label class="navbar-text me-2" for="id_email_input">
            Enter your email to log in
          </label>
          <input
            id="id_email_input"
            name="email"
            class="form-control"
            placeholder="your@email.com"
          />
          {% csrf_token %}
        </div>
      </form>
    {% endif %}
  </div>
</nav>
----
====

<1> Here's a new `{% if %}`, and navbar content for logged-in users.

OK, there's a to-do in there about the log-out button. We'll get to that, but how does our FT look now?


[subs="specialcharacters,macros"]
----
$ pass:quotes[*python src/manage.py test functional_tests.test_login*]
[...]
.
 ---------------------------------------------------------------------
Ran 1 test in 3.282s

OK
----



=== It Works in Theory!  Does It Work in Practice?


((("mocks", "practical application of")))
Wow! Can you believe it?  I scarcely can!
Time for a manual look around with `runserver`:


[role="skipme"]
[subs="specialcharacters,macros"]
----
$ pass:quotes[*python src/manage.py runserver*]
[...]
Internal Server Error: /accounts/send_login_email
Traceback (most recent call last):
  File "...goat-book/accounts/views.py", line 20, in send_login_email

ConnectionRefusedError: [Errno 111] Connection refused
----


==== Using Our New Environment Variable, and Saving It to .env

You'll probably get an error, like I did, when you try to run things manually.
It's because of two things.

Firstly, we need to re-add the email configuration to _settings.py_:

// DAVID: Shouldn't we write a failing test first? If not, why not?

[role="sourcecode"]
.src/superlists/settings.py (ch21l023)
====
[source,python]
----
EMAIL_HOST = "smtp.gmail.com"
EMAIL_HOST_USER = "obeythetestinggoat@gmail.com"
EMAIL_HOST_PASSWORD = os.environ.get("EMAIL_PASSWORD")
EMAIL_PORT = 587
EMAIL_USE_TLS = True
----
====

Secondly, we (probably) need to reset the `EMAIL_PASSWORD` in our shell:

[subs="specialcharacters,quotes"]
----
$ *export EMAIL_PASSWORD="yoursekritpasswordhere"*
----

.Using a Local .env File for Development
*******************************************************************************

Until now, we've not needed to "save" any of our local environment variables,
because the command-line ones are easy to remember and type,
and we've made sure all the other ones that affect config settings have sensible defaults for dev.
But there's just no way to get a working login system without this one!

Rather than having to go look up this password every time you start a new shell,
it's quite common to save these sorts of settings into a local file
in your project folder named `.env`.
It's a convention that makes it a hidden file, on Unix-like systems at least:

[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *echo .env >> .gitignore*  # we don't want to commit our secrets into git!
$ *echo 'EMAIL_PASSWORD="yoursekritpasswordhere"' >> .env*
$ *set -a; source .env; set +a;*
----

It does mean you have to remember to do that weird `set -a; source...` dance,
every time you start working on the project,
as well as remembering to activate your virtualenv.

If you search or ask around, you'll find there are some tools and shell plugins
that load virtualenvs and _.env_ files automatically, or Django plugins that handle this stuff too. A few options:

* Django-specific:
  https://django-environ.readthedocs.io[django-environ] or
  https://github.com/jpadilla/django-dotenv[django-dotenv]
* More general Python project management: https://docs.pipenv.org[Pipenv]
* Or even: https://oreil.ly/F9iV3[roll your own]

*******************************************************************************

And now...


[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *python src/manage.py runserver*
----

...you should see something like <<despiked-success-message>>.


[[despiked-success-message]]
.Check your email...
image::images/tdd3_2102.png["De-spiked site with success message"]

Woohoo!

I've been waiting to do a commit up until this moment, just to make sure
everything works.  At this point, you could make a series of separate
commits--one for the login view, one for the auth backend, one for
the user model, one for wiring up the template.  Or you could decide that—because they're all interrelated, and none will work without the others—you may as well just have one big commit:

[subs="specialcharacters,quotes"]
----
$ *git status*
$ *git add .*
$ *git diff --staged*
$ *git commit -m "Custom passwordless auth backend + custom user model"*
----

[role="scratchpad"]
*****
* _[strikethrough line-through]#Token model with email and UID#_
* _[strikethrough line-through]#View to create token and send login email incl. url w/ token UID#_
* _[strikethrough line-through]#Custom user model with USERNAME_FIELD=email#_
* _[strikethrough line-through]#Authentication backend with authenticate() and get_user() functions#_
* _[strikethrough line-through]#Registering auth backend in settings.py#_
* _[strikethrough line-through]#Login view calls authenticate() and login() from django.contrib.auth#_
* _Logout view calls django.contrib.auth.logout_
*****


=== Finishing Off Our FT: Testing Logout


((("mocks", "logout link")))
The last thing we need to do before we call it a day is to test the logout button.
We extend the FT with a couple more steps:

[role="sourcecode small-code"]
.src/functional_tests/test_login.py (ch21l024)
====
[source,python]
----
        [...]
        # she is logged in!
        self.wait_for(
            lambda: self.browser.find_element(By.CSS_SELECTOR, "#id_logout"),
        )
        navbar = self.browser.find_element(By.CSS_SELECTOR, ".navbar")
        self.assertIn(TEST_EMAIL, navbar.text)

        # Now she logs out
        self.browser.find_element(By.CSS_SELECTOR, "#id_logout").click()

        # She is logged out
        self.wait_for(
            lambda: self.browser.find_element(By.CSS_SELECTOR, "input[name=email]")
        )
        navbar = self.browser.find_element(By.CSS_SELECTOR, ".navbar")
        self.assertNotIn(TEST_EMAIL, navbar.text)
----
====

With that, we can see that the test is failing because the logout button
doesn't have a valid URL to submit to:

[subs=""]
----
$ <strong>python src/manage.py test functional_tests.test_login</strong>
[...]
selenium.common.exceptions.NoSuchElementException: Message: Unable to locate
element: input[name=email]; [...]
----


So, let's tell the base template that we want a new URL named "logout":

[role="sourcecode small-code"]
.src/lists/templates/base.html (ch21l025)
====
[source,html]
----
          {% if user.email %}
            <span class="navbar-text">Logged in as {{ user.email }}</span>
            <form method="POST" action="{% url 'logout' %}">
              {% csrf_token %}
              <button id="id_logout" class="btn btn-outline-secondary" type="submit">
                Log out
              </button>
            </form>
          {% else %}
----
====

If you try the FTs at this point,
you'll see an error saying that the URL doesn't exist yet:

[subs="specialcharacters,macros"]
----
$ pass:quotes[*python src/manage.py test functional_tests.test_login*]
Internal Server Error: /
[...]
django.urls.exceptions.NoReverseMatch: Reverse for 'logout' not found. 'logout'
is not a valid view function or pattern name.

======================================================================
ERROR: test_login_using_magic_link
(functional_tests.test_login.LoginTest.test_login_using_magic_link)
[...]

selenium.common.exceptions.NoSuchElementException: Message: Unable to locate
element: #id_logout; [...]
----



Implementing a logout URL is actually very simple:
we can use Django's
https://docs.djangoproject.com/en/5.2/topics/auth/default/#module-django.contrib.auth.views[built-in logout view],
which clears down the user's session and redirects them to a page of our choice:

[role="sourcecode small-code"]
.src/accounts/urls.py (ch21l026)
====
[source,python]
----
from django.contrib.auth import views as auth_views
from django.urls import path

from . import views

urlpatterns = [
    path("send_login_email", views.send_login_email, name="send_login_email"),
    path("login", views.login, name="login"),
    path("logout", auth_views.LogoutView.as_view(next_page="/"), name="logout"),
]
----
====


And that gets us a fully passing FT--indeed, a fully passing test suite:


[subs="specialcharacters,quotes"]
----
$ *python src/manage.py test functional_tests.test_login*
[...]
OK
$ *cd src && python manage.py test*
[...]
Ran 56 tests in 78.124s

OK
----


WARNING: We're nowhere near a truly secure or acceptable login system here.
    As this is just an example app for a book, we'll leave it at that,
    but in "real life" you'd want to explore a lot more security
    and usability issues before calling the job done.
    We're dangerously close to "rolling our own crypto" here,
    and relying on a more established login system would be much safer.
    Read more at https://security.stackexchange.com/a/18198.
    ((("security issues and settings", "login systems")))

// CSANAD: for demonstrating a security issue with our current, custom
// authentication, we could mention that after logout, we can log in using any
// of the previous login magic links (there is no token invalidation)

[role="scratchpad"]
*****
* _[strikethrough line-through]#Token model with email and UID#_
* _[strikethrough line-through]#View to create token and send login email incl. url w/ token UID#_
* _[strikethrough line-through]#Custom user model with USERNAME_FIELD=email#_
* _[strikethrough line-through]#Authentication backend with authenticate() and get_user() functions#_
* _[strikethrough line-through]#Registering auth backend in settings.py#_
* _[strikethrough line-through]#Login view calls authenticate() and login() from django.contrib.auth#_
* _[strikethrough line-through]#Logout view calls django.contrib.auth.logout#_
*****

In the next chapter, we'll start trying to put our login system to good use.
In the meantime, do a commit and enjoy this recap.

[role="pagebreak-before less_space"]
[[mocking-py-sidebar]]
.On Mocking in Python
*******************************************************************************

Using mock.return_value::
  The `.return_value` attribute on a mock can be used in two ways.
  You can _read_ it, to access the return value of a mocked-out function,
  and thus check on how it gets used later in your code;
  this usually happens in the "assert" or "then" part of your test.
  Alternatively, you can _assign_ to it,
  usally up-front in the "arrange" or "given" part of your test,
  as a way of saying
  "I want this mocked-out function to return a particular value".

Mocks can ensure test isolation and reduce duplication::
  You can use mocks to isolate different parts of your code from each other,
  and thus test them independently.
  This can help you to avoid duplication,
  because you're only testing a single layer at a time,
  rather than having to think about combinations of interactions
  of different layers.
  Used extensively, this approach leads to "London-style" TDD,
  but that's quite different from the style I mostly follow and show in this book.
  ((("mocks", "reducing duplication with")))
  ((("duplication, eliminating")))

Mocks can enable you to verify implementation details::
  Most tests should test behaviour, not implementation.
  At some point though, we decided using a particular implementation
  _was_ important. And so, we used a mock as a way to verify that,
  and to document it for our future selves.

There are alternatives to mocks, but they require rethinking how your code is structured::
  In a way, mocks make it "too easy".
  In programming languages
  that lack Python's dynamic ability to monkeypatch things at runtime,
  developers have had to work on alternative ways to test code with dependencies.
  While these techniques can be more complex,
  they do force you to think about how your code is structured—to cleanly identify your dependencies and build clean abstractions and interfaces around them.
  Further discussion is beyond the scope of this book,
  but check out http://cosmicpython.com[Cosmic Python].


*******************************************************************************
